{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import BatchNorm, MessagePassing, TopKPooling\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_scatter import scatter_mean\n",
    "from collections import defaultdict\n",
    "\n",
    "from custom.args import grey, purple\n",
    "from custom.dataset import GraphDataset, create_dataset\n",
    "from custom.utils import *\n",
    "from networkx.algorithms.centrality import degree_centrality\n",
    "import pickle\n",
    "from itertools import zip_longest\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.utils import degree, from_networkx, to_networkx\n",
    "\n",
    "\n",
    "from custom.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMatchingConvolution(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, args, aggr=\"add\"):\n",
    "        super(GraphMatchingConvolution, self).__init__(aggr=aggr)\n",
    "        self.args = args\n",
    "        self.lin_node = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_message = torch.nn.Linear(out_channels * 2, out_channels)\n",
    "        self.lin_passing = torch.nn.Linear(out_channels + in_channels, out_channels)\n",
    "        self.batch_norm = BatchNorm(out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x_transformed = self.lin_node(x)\n",
    "        return self.propagate(edge_index, x=x_transformed, original_x=x, batch=batch)\n",
    "\n",
    "    def message(self, edge_index_i, x_i, x_j):\n",
    "        x = torch.cat([x_i, x_j], dim=1)\n",
    "        m = self.lin_message(x)\n",
    "        return m\n",
    "\n",
    "    def update(self, aggr_out, edge_index, x, original_x, batch):\n",
    "        n_graphs = torch.unique(batch).shape[0]\n",
    "        cross_graph_attention, a_x, a_y = batch_block_pair_attention(\n",
    "            original_x, batch, n_graphs\n",
    "        )\n",
    "        attention_input = original_x - cross_graph_attention\n",
    "        aggr_out = self.lin_passing(torch.cat([aggr_out, attention_input], dim=1))\n",
    "        aggr_out = self.batch_norm(aggr_out)\n",
    "\n",
    "        norms = torch.norm(aggr_out, p=2, dim=1)\n",
    "        cross_attention_sums = cross_graph_attention.sum(dim=1)\n",
    "\n",
    "        return (\n",
    "            aggr_out,\n",
    "            edge_index,\n",
    "            batch,\n",
    "            (\n",
    "                attention_input,\n",
    "                cross_graph_attention,\n",
    "                a_x,\n",
    "                a_y,\n",
    "                norms,\n",
    "                cross_attention_sums,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class GraphAggregator(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, args):\n",
    "        super(GraphAggregator, self).__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_gate = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_final = torch.nn.Linear(out_channels, out_channels)\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x_states = self.lin(x)\n",
    "        x_gates = torch.nn.functional.softmax(self.lin_gate(x), dim=1)\n",
    "        x_states = x_states * x_gates\n",
    "        x_states = scatter_mean(x_states, batch, dim=0)\n",
    "        x_states = self.lin_final(x_states)\n",
    "        return x_states\n",
    "\n",
    "\n",
    "class GraphMatchingNetwork(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GraphMatchingNetwork, self).__init__()\n",
    "        self.args = args\n",
    "        self.margin = self.args.margin\n",
    "        if args.n_classes > 2:\n",
    "            self.f1_average = \"micro\"\n",
    "        else:\n",
    "            self.f1_average = \"binary\"\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(\n",
    "            GraphMatchingConvolution(self.args.feat_dim, self.args.dim, args)\n",
    "        )\n",
    "        for _ in range(self.args.num_layers - 1):\n",
    "            self.layers.append(\n",
    "                GraphMatchingConvolution(self.args.dim, self.args.dim, args)\n",
    "            )\n",
    "        self.aggregator = GraphAggregator(self.args.dim, self.args.dim, self.args)\n",
    "        self.layer_outputs = []\n",
    "        self.layer_cross_attentions = []\n",
    "        self.mincut = []\n",
    "        self.mlp = torch.nn.Sequential()\n",
    "        self.args.n_clusters = args.n_clusters\n",
    "        self.mlp.append(Linear(self.args.dim, self.args.n_clusters))\n",
    "        self.topk_outputs = []\n",
    "        self.norms_per_layer = []\n",
    "        self.attention_sums_per_layer = []\n",
    "\n",
    "    def compute_emb(\n",
    "        self, feats, edge_index, batch, sizes_1, sizes_2, edge_index_1, edge_index_2\n",
    "    ):\n",
    "\n",
    "        topk_pooling = TopKPooling(\n",
    "            self.args.dim, ratio=min(sizes_1.item(), sizes_2.item())\n",
    "        )\n",
    "        for i in range(self.args.num_layers):\n",
    "            (\n",
    "                feats,\n",
    "                edge_index,\n",
    "                batch,\n",
    "                (\n",
    "                    attention_input,\n",
    "                    cross_graph_attention,\n",
    "                    a_x,\n",
    "                    a_y,\n",
    "                    norms,\n",
    "                    attention_sums,\n",
    "                ),\n",
    "            ) = self.layers[i](feats, edge_index, batch)\n",
    "\n",
    "            x_1 = feats[: sizes_1.item(), :]\n",
    "            x_2 = feats[sizes_1.item() : sizes_1.item() + sizes_2.item(), :]\n",
    "\n",
    "            norms_1 = norms[: sizes_1.item()]\n",
    "            norms_2 = norms[sizes_1.item() : sizes_1.item() + sizes_2.item()]\n",
    "\n",
    "            attention_sums_1 = attention_sums[: sizes_1.item()]\n",
    "            attention_sums_2 = attention_sums[\n",
    "                sizes_1.item() : sizes_1.item() + sizes_2.item()\n",
    "            ]\n",
    "\n",
    "            x_pooled_1, edge_index_pooled_1, _, _, perm1, score1 = topk_pooling(\n",
    "                x_1,\n",
    "                edge_index_1,\n",
    "            )\n",
    "            x_pooled_2, edge_index_pooled_2, _, _, perm2, score2 = topk_pooling(\n",
    "                x_2,\n",
    "                edge_index_2,\n",
    "            )\n",
    "\n",
    "            self.topk_outputs.append(\n",
    "                (\n",
    "                    (x_pooled_1, edge_index_pooled_1, perm1, score1),\n",
    "                    (x_pooled_2, edge_index_pooled_2, perm2, score2),\n",
    "                )\n",
    "            )\n",
    "            self.layer_cross_attentions.append((cross_graph_attention, a_x, a_y))\n",
    "            self.layer_outputs.append((x_1, edge_index_1, x_2, edge_index_2))\n",
    "            self.norms_per_layer.append((norms_1, norms_2))\n",
    "            self.attention_sums_per_layer.append((attention_sums_1, attention_sums_2))\n",
    "\n",
    "        feats = self.aggregator(feats, edge_index, batch)\n",
    "        return feats, edge_index, batch\n",
    "\n",
    "    def combine_pair_embedding(\n",
    "        self, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "    ):\n",
    "        feats = torch.cat([feats_1, feats_2], dim=0)\n",
    "        max_node_idx_1 = sizes_1.sum()\n",
    "        edge_index_2_offset = edge_index_2 + max_node_idx_1\n",
    "        edge_index = torch.cat([edge_index_1, edge_index_2_offset], dim=1)\n",
    "        batch = create_batch(torch.cat([sizes_1, sizes_2], dim=0))\n",
    "        feats, edge_index, batch = (\n",
    "            feats.to(self.args.device),\n",
    "            edge_index.to(self.args.device),\n",
    "            batch.to(self.args.device),\n",
    "        )\n",
    "        return feats, edge_index, batch\n",
    "\n",
    "    def forward(self, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2):\n",
    "        self.layer_outputs = []\n",
    "        self.layer_cross_attentions = []\n",
    "        self.topk_outputs = []\n",
    "        self.mincut = []\n",
    "        self.norms_per_layer = []\n",
    "        self.attention_sums_per_layer = []\n",
    "\n",
    "        feats, edge_index, batch = self.combine_pair_embedding(\n",
    "            feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "        )\n",
    "        emb, _, _ = self.compute_emb(\n",
    "            feats, edge_index, batch, sizes_1, sizes_2, edge_index_1, edge_index_2\n",
    "        )\n",
    "        emb_1 = emb[: emb.shape[0] // 2, :]\n",
    "        emb_2 = emb[emb.shape[0] // 2 :, :]\n",
    "\n",
    "        best_acc1, best_acc2 = 0.0, 0.0\n",
    "        cluster1, cluster2 = None, None\n",
    "        layer1, layer2 = None, None\n",
    "        clusters = []\n",
    "\n",
    "        return emb_1, emb_2, cluster1, cluster2, layer1, layer2\n",
    "\n",
    "    def compute_metrics(self, emb_1, emb_2, labels):\n",
    "        distances = torch.norm(emb_1 - emb_2, p=2, dim=1)\n",
    "        loss = F.relu(self.margin - labels * (1 - distances)).mean()\n",
    "        predicted_similar = torch.where(\n",
    "            distances < self.args.margin,\n",
    "            torch.ones_like(labels),\n",
    "            -torch.ones_like(labels),\n",
    "        )\n",
    "        acc = (predicted_similar == labels).float().mean()\n",
    "        metrics = {\"loss\": loss, \"acc\": acc}\n",
    "        return metrics\n",
    "\n",
    "    def init_metric_dict(self):\n",
    "        return {\"acc\": -1, \"f1\": -1}\n",
    "\n",
    "    def has_improved(self, m1, m2):\n",
    "        return m1[\"acc\"] < m2[\"acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, pairs, labels, batch_size, title):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    losses = []\n",
    "    accs = []\n",
    "\n",
    "    for i in range(len(pairs)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        graph1, graph2 = pairs[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "        feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "        sizes_1 = torch.tensor([graph1.num_nodes])\n",
    "        sizes_2 = torch.tensor([graph2.num_nodes])\n",
    "\n",
    "        emb_1, emb_2, _, _, _, _ = model(\n",
    "            feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "        )\n",
    "\n",
    "        metrics = model.compute_metrics(emb_1, emb_2, torch.tensor([label]))\n",
    "        loss = metrics[\"loss\"]\n",
    "        acc = metrics[\"acc\"]\n",
    "\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "\n",
    "        if i % batch_size == 0 and i > 0:\n",
    "            batch_loss = torch.mean(torch.stack(losses))\n",
    "            batch_acc = torch.mean(torch.stack(accs))\n",
    "            losses = []\n",
    "            accs = []\n",
    "            train_losses.append(batch_loss.detach().numpy())\n",
    "            train_accuracies.append(batch_acc.detach().numpy())\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # if train_accuracies[-1] > 0.0:\n",
    "    #     plt.figure(figsize=(12, 5))\n",
    "    #     plt.title(title)\n",
    "\n",
    "    #     plt.subplot(1, 2, 1)\n",
    "    #     plt.plot(train_losses, label=\"Training Loss\")\n",
    "    #     plt.title(\"Loss over Epochs\")\n",
    "    #     plt.xlabel(\"Epochs\")\n",
    "    #     plt.ylabel(\"Loss\")\n",
    "    #     plt.legend()\n",
    "\n",
    "    #     plt.subplot(1, 2, 2)\n",
    "    #     plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "    #     plt.title(\"Accuracy over Epochs\")\n",
    "    #     plt.xlabel(\"Epochs\")\n",
    "    #     plt.ylabel(\"Accuracy\")\n",
    "    #     plt.legend()\n",
    "\n",
    "    #     plt.show()\n",
    "\n",
    "    # return train_accuracies[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import degree\n",
    "import time\n",
    "\n",
    "\n",
    "class MCS:\n",
    "    def __init__(self, mp):\n",
    "        self.mp = mp\n",
    "        self.max_size = 0\n",
    "        self.all_mappings = []\n",
    "        self.unique_mappings = set()\n",
    "        self.visited = set()\n",
    "        self.time_limit = 5\n",
    "        self.start_time = None\n",
    "\n",
    "    def find_mcs(self, G1, G2):\n",
    "        self.max_size = 0\n",
    "        self.all_mappings = []\n",
    "        self.unique_mappings = set()\n",
    "        self.visited = set()\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        G1_degrees = degree(G1.edge_index[0], G1.num_nodes)\n",
    "        G2_degrees = degree(G2.edge_index[0], G2.num_nodes)\n",
    "\n",
    "        nodes1 = list(range(G1.num_nodes))\n",
    "        nodes2 = list(range(G2.num_nodes))\n",
    "\n",
    "        for n1 in nodes1:\n",
    "            for n2 in nodes2:\n",
    "                if (n1, n2) in self.mp:\n",
    "                    M = {n1: n2}\n",
    "                    neighbors1 = set(G1.edge_index[1][G1.edge_index[0] == n1].tolist())\n",
    "                    neighbors2 = set(G2.edge_index[1][G2.edge_index[0] == n2].tolist())\n",
    "                    self.match(\n",
    "                        G1, G2, M, G1_degrees, G2_degrees, neighbors1, neighbors2\n",
    "                    )\n",
    "        return self.all_mappings\n",
    "\n",
    "    def match(self, G1, G2, M, G1_degrees, G2_degrees, neighbors1, neighbors2):\n",
    "        if time.time() - self.start_time > self.time_limit:\n",
    "            return\n",
    "\n",
    "        state = (frozenset(M.items()), frozenset(neighbors1), frozenset(neighbors2))\n",
    "        if state in self.visited:\n",
    "            return\n",
    "        self.visited.add(state)\n",
    "\n",
    "        edge_count = self.count_edges(M, G1, G2)\n",
    "\n",
    "        if len(M) > self.max_size or (\n",
    "            len(M) == self.max_size and edge_count > self.edge_count\n",
    "        ):\n",
    "            self.max_size = len(M)\n",
    "            self.edge_count = edge_count\n",
    "            self.all_mappings = [M.copy()]\n",
    "            self.unique_mappings = {self.canonical_form(M)}\n",
    "        elif len(M) == self.max_size and edge_count == self.edge_count:\n",
    "            canonical = self.canonical_form(M)\n",
    "            if canonical not in self.unique_mappings:\n",
    "                self.all_mappings.append(M.copy())\n",
    "                self.unique_mappings.add(canonical)\n",
    "\n",
    "        candidates1 = sorted(neighbors1, key=lambda n: -G1_degrees[n].item())\n",
    "        candidates2 = sorted(neighbors2, key=lambda n: -G2_degrees[n].item())\n",
    "\n",
    "        for n1 in candidates1:\n",
    "            if n1 not in M:\n",
    "                for n2 in candidates2:\n",
    "                    if n2 not in M.values() and self.feasible(n1, n2, M, G1, G2):\n",
    "                        M[n1] = n2\n",
    "                        new_neighbors1 = set(\n",
    "                            G1.edge_index[1][G1.edge_index[0] == n1].tolist()\n",
    "                        )\n",
    "                        new_neighbors2 = set(\n",
    "                            G2.edge_index[1][G2.edge_index[0] == n2].tolist()\n",
    "                        )\n",
    "                        neighbors1.update(new_neighbors1 - set(M.keys()))\n",
    "                        neighbors2.update(new_neighbors2 - set(M.values()))\n",
    "                        self.match(\n",
    "                            G1,\n",
    "                            G2,\n",
    "                            M,\n",
    "                            G1_degrees,\n",
    "                            G2_degrees,\n",
    "                            neighbors1,\n",
    "                            neighbors2,\n",
    "                        )\n",
    "                        del M[n1]\n",
    "                        neighbors1.difference_update(new_neighbors1)\n",
    "                        neighbors2.difference_update(new_neighbors2)\n",
    "\n",
    "    def feasible(self, n1, n2, M, G1, G2):\n",
    "        if not torch.equal(G1.x[n1], G2.x[n2]):\n",
    "            return False\n",
    "        if (n1, n2) not in self.mp:\n",
    "            return False\n",
    "\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "\n",
    "        for neighbor in G1.edge_index[1][G1.edge_index[0] == n1]:\n",
    "            if neighbor.item() in M:\n",
    "                count1 += 1\n",
    "\n",
    "        for neighbor in G2.edge_index[1][G2.edge_index[0] == n2]:\n",
    "            if neighbor.item() in M.values():\n",
    "                count2 += 1\n",
    "\n",
    "        if count1 != count2:\n",
    "            return False\n",
    "\n",
    "        for neighbor in G1.edge_index[1][G1.edge_index[0] == n1]:\n",
    "            if (\n",
    "                neighbor.item() in M\n",
    "                and M[neighbor.item()]\n",
    "                not in G2.edge_index[1][G2.edge_index[0] == n2].tolist()\n",
    "            ):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def count_edges(self, M, G1, G2):\n",
    "        edge_count = 0\n",
    "        for u1, v1 in M.items():\n",
    "            for u2, v2 in M.items():\n",
    "                if u1 != u2:\n",
    "                    u1_v1_exists = (\n",
    "                        (G1.edge_index[0] == u1) & (G1.edge_index[1] == u2)\n",
    "                    ).any() or (\n",
    "                        (G1.edge_index[0] == u2) & (G1.edge_index[1] == u1)\n",
    "                    ).any()\n",
    "                    u2_v2_exists = (\n",
    "                        (G2.edge_index[0] == v1) & (G2.edge_index[1] == v2)\n",
    "                    ).any() or (\n",
    "                        (G2.edge_index[0] == v2) & (G2.edge_index[1] == v1)\n",
    "                    ).any()\n",
    "                    if u1_v1_exists and u2_v2_exists:\n",
    "                        edge_count += 1\n",
    "        return edge_count\n",
    "\n",
    "    def canonical_form(self, M):\n",
    "        G1_set = set(M.keys())\n",
    "        G2_set = set(M.values())\n",
    "        return (frozenset(G1_set), frozenset(G2_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attentions(graph1, graph2, attention_pairs, title=\"\"):\n",
    "    def get_node_labels(graph):\n",
    "        node_labels = {}\n",
    "        for node in range(graph.num_nodes):\n",
    "            node_labels[node] = node\n",
    "        return node_labels\n",
    "\n",
    "    G1 = to_networkx(graph1, to_undirected=True)\n",
    "    G2 = to_networkx(graph2, to_undirected=True)\n",
    "\n",
    "    G_combined = nx.Graph()\n",
    "\n",
    "    for n, d in G1.nodes(data=True):\n",
    "        G_combined.add_node(n, **d)\n",
    "    for n, d in G2.nodes(data=True):\n",
    "        G_combined.add_node(n + len(G1.nodes), **d)\n",
    "\n",
    "    G_combined.add_edges_from([(u, v) for u, v in G1.edges()])\n",
    "    G_combined.add_edges_from(\n",
    "        [(u + len(G1.nodes), v + len(G1.nodes)) for u, v in G2.edges()]\n",
    "    )\n",
    "\n",
    "    G_plain = G_combined.copy()\n",
    "\n",
    "    for node1, node2 in attention_pairs:\n",
    "        G_combined.add_edge(node1, node2 + len(G1.nodes))\n",
    "\n",
    "    pos_G1 = nx.spring_layout(G1)\n",
    "    pos_G2 = nx.spring_layout(G2)\n",
    "\n",
    "    for key in pos_G1.keys():\n",
    "        pos_G1[key] = [pos_G1[key][0] - 1.5, pos_G1[key][1]]\n",
    "\n",
    "    for key in pos_G2.keys():\n",
    "        pos_G2[key] = [pos_G2[key][0] + 1.5, pos_G2[key][1]]\n",
    "\n",
    "    pos_combined = {**pos_G1, **{k + len(G1.nodes): v for k, v in pos_G2.items()}}\n",
    "\n",
    "    node_labels_G1 = get_node_labels(graph1)\n",
    "    node_labels_G2 = get_node_labels(graph2)\n",
    "    node_labels_combined = {\n",
    "        **node_labels_G1,\n",
    "        **{k + len(G1.nodes): v for k, v in node_labels_G2.items()},\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.gcf().patch.set_alpha(0)\n",
    "\n",
    "    nx.draw(\n",
    "        G_plain,\n",
    "        pos=pos_combined,\n",
    "        with_labels=False,\n",
    "        labels=node_labels_combined,\n",
    "        node_color=\"#3b8bc2\",\n",
    "        edge_color=\"black\",\n",
    "        node_size=500,\n",
    "    )\n",
    "\n",
    "    # attention_edges = [\n",
    "    #     (node1, node2 + len(G1.nodes)) for node1, node2 in attention_pairs\n",
    "    # ]\n",
    "    # nx.draw_networkx_edges(\n",
    "    #     G_combined,\n",
    "    #     pos=pos_combined,\n",
    "    #     edgelist=attention_edges,\n",
    "    #     edge_color=\"lightgrey\",\n",
    "    # )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.gcf().patch.set_alpha(0)\n",
    "\n",
    "    nx.draw(\n",
    "        G_combined,\n",
    "        pos=pos_combined,\n",
    "        with_labels=False,\n",
    "        labels=node_labels_combined,\n",
    "        node_color=\"#3b8bc2\",\n",
    "        edge_color=\"black\",\n",
    "        node_size=500,\n",
    "    )\n",
    "\n",
    "    attention_edges = [\n",
    "        (node1, node2 + len(G1.nodes)) for node1, node2 in attention_pairs\n",
    "    ]\n",
    "    nx.draw_networkx_edges(\n",
    "        G_combined,\n",
    "        pos=pos_combined,\n",
    "        edgelist=attention_edges,\n",
    "        edge_color=\"lightgrey\",\n",
    "    )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(graph1, graph2, attentions, t):\n",
    "    most_nodes = 0\n",
    "    largest_summary = None\n",
    "    for i in range(3):\n",
    "        attention_nodes = extract_dynamic_attention_nodes(attentions, threshold=t)\n",
    "        mp = mutual_pairs(attention_nodes, i)\n",
    "        vf2 = MCS(mp)\n",
    "        patterns = vf2.find_mcs(graph1, graph2)\n",
    "\n",
    "        if patterns != []:\n",
    "            pattern = patterns[0]\n",
    "\n",
    "            g1_subgraph, g2_subgraph = create_subgraphs(pattern, graph1, graph2)\n",
    "\n",
    "            if (\n",
    "                nx.is_isomorphic(\n",
    "                    to_networkx(g1_subgraph, to_undirected=True),\n",
    "                    to_networkx(g2_subgraph, to_undirected=True),\n",
    "                )\n",
    "                and len(pattern) > 2\n",
    "            ):\n",
    "                summary = g1_subgraph\n",
    "                # plot_mutag(summary)\n",
    "\n",
    "                if len(pattern) > most_nodes:\n",
    "                    most_nodes = len(pattern)\n",
    "                    largest_summary = summary\n",
    "                    final_pattern = pattern\n",
    "\n",
    "    return largest_summary, final_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def load_graphs(name, c=\"cycle\"):\n",
    "    with open(f\"gnnexplainer_graphs/graph{i}_{c}.pkl\", \"rb\") as f:\n",
    "        original_G, _ = pickle.load(f)\n",
    "\n",
    "    for node in original_G.nodes:\n",
    "        if \"self\" in original_G.nodes[node]:\n",
    "            del original_G.nodes[node][\"self\"]\n",
    "\n",
    "    data = from_networkx(original_G)\n",
    "    data.x = data.feat.float()\n",
    "    # plot_graph(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "cycles = []\n",
    "completes = []\n",
    "lines = []\n",
    "stars = []\n",
    "for i in range(10):\n",
    "    cycles.append(load_graphs(i, \"cycle\"))\n",
    "    completes.append(load_graphs(i, \"complete\"))\n",
    "    lines.append(load_graphs(i, \"line\"))\n",
    "    stars.append(load_graphs(i, \"star\"))\n",
    "\n",
    "# (graph1, graph2, attentions) = torch.load(\n",
    "#     \"examples/method2_oof_v2_exact_cycle_8.pt\", weights_only=False\n",
    "# )\n",
    "\n",
    "# summary, final_pattern = loop(graph1, graph1, attentions, 0.08)\n",
    "\n",
    "# print(final_pattern)\n",
    "\n",
    "# print(summary.x)\n",
    "# plot_graph_pair(graph1, graph2)\n",
    "# plot_graph(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 1), (9, 2), (8, 4), (5, 4), (0, 8), (7, 9), (6, 0), (7, 2), (3, 1), (5, 3)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "numbers = list(range(10)) * 2\n",
    "random.shuffle(numbers)\n",
    "random_pairs = [(numbers[i], numbers[i + 1]) for i in range(0, len(numbers), 2)]\n",
    "\n",
    "print(random_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/st/cyh3v8ln22sdswp1s386w5vw0000gn/T/ipykernel_21764/4091382530.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = GraphDataset(torch.load(\"data/cycle_line_star_complete_1.pt\"))\n"
     ]
    }
   ],
   "source": [
    "dataset = GraphDataset(torch.load(\"data/cycle_line_star_complete_1.pt\"))\n",
    "\n",
    "\n",
    "class NewArgs:\n",
    "    def __init__(self, dim, num_layers, margin, lr, batch_size, num_pairs):\n",
    "        self.dim = dim\n",
    "        self.feat_dim = dataset.num_features\n",
    "        self.num_layers = num_layers\n",
    "        self.margin = margin\n",
    "        self.lr = lr\n",
    "        self.n_classes = dataset.num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.n_clusters = 8\n",
    "        self.num_pairs = num_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = (32, 7, 0.2, 0.01, 64, 500)\n",
    "newargs = NewArgs(*hyperparams)\n",
    "model = GraphMatchingNetwork(newargs)\n",
    "optimizer = Adam(model.parameters(), lr=newargs.lr, weight_decay=1e-5)\n",
    "pairs, labels = create_graph_pairs(dataset, newargs.num_pairs)\n",
    "train(model, optimizer, pairs, labels, newargs.batch_size, str(hyperparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m cycles_prototypes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 81\u001b[0m     cycles_prototypes\u001b[38;5;241m.\u001b[39mappend(\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_pairs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     83\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m cycles_prototypes:\n",
      "Cell \u001b[0;32mIn[11], line 39\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(n, l, random_pairs, t)\u001b[0m\n\u001b[1;32m     37\u001b[0m mp \u001b[38;5;241m=\u001b[39m mutual_pairs(attention_nodes, i)\n\u001b[1;32m     38\u001b[0m vf2 \u001b[38;5;241m=\u001b[39m MCS(mp)\n\u001b[0;32m---> 39\u001b[0m patterns \u001b[38;5;241m=\u001b[39m \u001b[43mvf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_mcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m patterns \u001b[38;5;241m!=\u001b[39m [] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(patterns[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     41\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m patterns[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mMCS.find_mcs\u001b[0;34m(self, G1, G2)\u001b[0m\n\u001b[1;32m     34\u001b[0m             neighbors1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(G1\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m1\u001b[39m][G1\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m n1]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     35\u001b[0m             neighbors2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(G2\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m1\u001b[39m][G2\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m n2]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m---> 36\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                \u001b[49m\u001b[43mG1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG1_degrees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG2_degrees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbors1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbors2\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_mappings\n",
      "Cell \u001b[0;32mIn[4], line 81\u001b[0m, in \u001b[0;36mMCS.match\u001b[0;34m(self, G1, G2, M, G1_degrees, G2_degrees, neighbors1, neighbors2)\u001b[0m\n\u001b[1;32m     79\u001b[0m neighbors1\u001b[38;5;241m.\u001b[39mupdate(new_neighbors1 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(M\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     80\u001b[0m neighbors2\u001b[38;5;241m.\u001b[39mupdate(new_neighbors2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(M\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG1_degrees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG2_degrees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m M[n1]\n\u001b[1;32m     91\u001b[0m neighbors1\u001b[38;5;241m.\u001b[39mdifference_update(new_neighbors1)\n",
      "Cell \u001b[0;32mIn[4], line 81\u001b[0m, in \u001b[0;36mMCS.match\u001b[0;34m(self, G1, G2, M, G1_degrees, G2_degrees, neighbors1, neighbors2)\u001b[0m\n\u001b[1;32m     79\u001b[0m neighbors1\u001b[38;5;241m.\u001b[39mupdate(new_neighbors1 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(M\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     80\u001b[0m neighbors2\u001b[38;5;241m.\u001b[39mupdate(new_neighbors2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(M\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG1_degrees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG2_degrees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m M[n1]\n\u001b[1;32m     91\u001b[0m neighbors1\u001b[38;5;241m.\u001b[39mdifference_update(new_neighbors1)\n",
      "    \u001b[0;31m[... skipping similar frames: MCS.match at line 81 (1 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 81\u001b[0m, in \u001b[0;36mMCS.match\u001b[0;34m(self, G1, G2, M, G1_degrees, G2_degrees, neighbors1, neighbors2)\u001b[0m\n\u001b[1;32m     79\u001b[0m neighbors1\u001b[38;5;241m.\u001b[39mupdate(new_neighbors1 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(M\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     80\u001b[0m neighbors2\u001b[38;5;241m.\u001b[39mupdate(new_neighbors2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(M\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG1_degrees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG2_degrees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m M[n1]\n\u001b[1;32m     91\u001b[0m neighbors1\u001b[38;5;241m.\u001b[39mdifference_update(new_neighbors1)\n",
      "Cell \u001b[0;32mIn[4], line 71\u001b[0m, in \u001b[0;36mMCS.match\u001b[0;34m(self, G1, G2, M, G1_degrees, G2_degrees, neighbors1, neighbors2)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n1 \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m M:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n2 \u001b[38;5;129;01min\u001b[39;00m candidates2:\n\u001b[0;32m---> 71\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n2 \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m M\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeasible\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG2\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     72\u001b[0m             M[n1] \u001b[38;5;241m=\u001b[39m n2\n\u001b[1;32m     73\u001b[0m             new_neighbors1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m     74\u001b[0m                 G1\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m1\u001b[39m][G1\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m n1]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     75\u001b[0m             )\n",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m, in \u001b[0;36mMCS.feasible\u001b[0;34m(self, n1, n2, M, G1, G2)\u001b[0m\n\u001b[1;32m    100\u001b[0m count1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    101\u001b[0m count2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m neighbor \u001b[38;5;129;01min\u001b[39;00m G1\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m1\u001b[39m][G1\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m n1]:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m neighbor\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;129;01min\u001b[39;00m M:\n\u001b[1;32m    105\u001b[0m         count1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/_tensor.py:1033\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing len to get tensor shape might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1025\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended usage would be tensor.shape[0]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1029\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1030\u001b[0m         )\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# save us work, and also helps keep trace ordering deterministic\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;66;03m# (e.g., if you zip(*hiddens), the eager map will force all the\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;66;03m# indexes of hiddens[0] before hiddens[1], while the generator\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;66;03m# map will interleave them.)\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# See gh-54457\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration over a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run(n, l, random_pairs, t=0.08):\n",
    "    idx1, idx2 = random_pairs[n]\n",
    "    graph1, graph2 = l[idx1], l[idx2]\n",
    "\n",
    "    feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "    feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "    sizes_1 = torch.tensor([len(graph1.x)])\n",
    "    sizes_2 = torch.tensor([len(graph2.x)])\n",
    "    emb1, emb2, cluster1, cluster2, layer1, layer2 = model(\n",
    "        feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "    )\n",
    "\n",
    "    embeddings, attentions = extract_embeddings_and_attention(\n",
    "        model, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "    )\n",
    "\n",
    "    br = False\n",
    "\n",
    "    most_nodes = 0\n",
    "    largest_summary = None\n",
    "    t_of_largest = 0\n",
    "    layer_of_largest = 0\n",
    "    ts = []\n",
    "    mps = None\n",
    "\n",
    "    mps = None\n",
    "    most_nodes = 0\n",
    "    largest_summary = None\n",
    "    t_of_largest = 0\n",
    "    layer_of_largest = 0\n",
    "    ts = []\n",
    "    mps = None\n",
    "\n",
    "    mps = None\n",
    "    for i in range(7):\n",
    "        attention_nodes = extract_dynamic_attention_nodes(attentions, threshold=t)\n",
    "        mp = mutual_pairs(attention_nodes, i)\n",
    "        vf2 = MCS(mp)\n",
    "        patterns = vf2.find_mcs(graph1, graph2)\n",
    "        if patterns != [] and len(patterns[0]) > 2:\n",
    "            pattern = patterns[0]\n",
    "\n",
    "            g1_subgraph, g2_subgraph = create_subgraphs(pattern, graph1, graph2)\n",
    "\n",
    "            if (\n",
    "                nx.is_isomorphic(\n",
    "                    to_networkx(g1_subgraph, to_undirected=True),\n",
    "                    to_networkx(g2_subgraph, to_undirected=True),\n",
    "                )\n",
    "                and len(pattern) > 2\n",
    "            ):\n",
    "\n",
    "                summary = g1_subgraph\n",
    "\n",
    "                if len(pattern) > most_nodes:\n",
    "                    most_nodes = len(pattern)\n",
    "                    largest_summary = summary\n",
    "                    t_of_largest = t\n",
    "                    layer_of_largest = i + 1\n",
    "                    mps = mp\n",
    "\n",
    "    # plot_graph_pair(graph1, graph2)\n",
    "    # plot_graph(largest_summary)\n",
    "    return largest_summary, idx1, idx2\n",
    "\n",
    "\n",
    "while True:\n",
    "    hyperparams = (32, 7, 0.2, 0.01, 64, 500)\n",
    "    newargs = NewArgs(*hyperparams)\n",
    "    model = GraphMatchingNetwork(newargs)\n",
    "    optimizer = Adam(model.parameters(), lr=newargs.lr, weight_decay=1e-5)\n",
    "    pairs, labels = create_graph_pairs(dataset, newargs.num_pairs)\n",
    "    train(model, optimizer, pairs, labels, newargs.batch_size, str(hyperparams))\n",
    "\n",
    "    numbers = list(range(10)) * 2\n",
    "    random.shuffle(numbers)\n",
    "    random_pairs = [(numbers[i], numbers[i + 1]) for i in range(0, len(numbers), 2)]\n",
    "\n",
    "    cycles_prototypes = []\n",
    "    for i in range(10):\n",
    "        cycles_prototypes.append(run(i, cycles, random_pairs))\n",
    "\n",
    "    counter = 0\n",
    "    for i in cycles_prototypes:\n",
    "        if is_cycle(i[0]):\n",
    "            counter += 1\n",
    "\n",
    "    if counter >= 5:\n",
    "        torch.save(cycles_prototypes, \"finally.pt\")\n",
    "        print(\"OOOOO\")\n",
    "        break\n",
    "\n",
    "# completes_prototypes = []\n",
    "# for i in range(10):\n",
    "#     completes_prototypes.append(run(i, completes, 0.1))\n",
    "\n",
    "# lines_prototypes = []\n",
    "# for i in range(10):\n",
    "#     lines_prototypes.append(run(i, lines))\n",
    "\n",
    "# stars_prototypes = []\n",
    "# for i in range(10):\n",
    "#     stars_prototypes.append(run(i, stars))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
