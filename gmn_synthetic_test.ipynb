{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import BatchNorm, MessagePassing, TopKPooling\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "from custom.args import grey, purple\n",
    "from custom.dataset import GraphDataset, create_dataset\n",
    "from custom.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMatchingConvolution(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, args, aggr=\"add\"):\n",
    "        super(GraphMatchingConvolution, self).__init__(aggr=aggr)\n",
    "        self.args = args\n",
    "        self.lin_node = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_message = torch.nn.Linear(out_channels * 2, out_channels)\n",
    "        self.lin_passing = torch.nn.Linear(out_channels + in_channels, out_channels)\n",
    "        self.batch_norm = BatchNorm(out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x_transformed = self.lin_node(x)\n",
    "        return self.propagate(edge_index, x=x_transformed, original_x=x, batch=batch)\n",
    "\n",
    "    def message(self, edge_index_i, x_i, x_j):\n",
    "        x = torch.cat([x_i, x_j], dim=1)\n",
    "        m = self.lin_message(x)\n",
    "        return m\n",
    "\n",
    "    def update(self, aggr_out, edge_index, x, original_x, batch):\n",
    "        n_graphs = torch.unique(batch).shape[0]\n",
    "        cross_graph_attention, a_x, a_y = batch_block_pair_attention(\n",
    "            original_x, batch, n_graphs\n",
    "        )\n",
    "        attention_input = original_x - cross_graph_attention\n",
    "        aggr_out = self.lin_passing(torch.cat([aggr_out, attention_input], dim=1))\n",
    "        aggr_out = self.batch_norm(aggr_out)\n",
    "\n",
    "        norms = torch.norm(aggr_out, p=2, dim=1)\n",
    "        cross_attention_sums = cross_graph_attention.sum(dim=1)\n",
    "\n",
    "        return (\n",
    "            aggr_out,\n",
    "            edge_index,\n",
    "            batch,\n",
    "            (\n",
    "                attention_input,\n",
    "                cross_graph_attention,\n",
    "                a_x,\n",
    "                a_y,\n",
    "                norms,\n",
    "                cross_attention_sums,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class GraphAggregator(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, args):\n",
    "        super(GraphAggregator, self).__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_gate = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_final = torch.nn.Linear(out_channels, out_channels)\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x_states = self.lin(x)\n",
    "        x_gates = torch.nn.functional.softmax(self.lin_gate(x), dim=1)\n",
    "        x_states = x_states * x_gates\n",
    "        x_states = scatter_mean(x_states, batch, dim=0)\n",
    "        x_states = self.lin_final(x_states)\n",
    "        return x_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMatchingNetwork(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GraphMatchingNetwork, self).__init__()\n",
    "        self.args = args\n",
    "        self.margin = self.args.margin\n",
    "        if args.n_classes > 2:\n",
    "            self.f1_average = \"micro\"\n",
    "        else:\n",
    "            self.f1_average = \"binary\"\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(\n",
    "            GraphMatchingConvolution(self.args.feat_dim, self.args.dim, args)\n",
    "        )\n",
    "        for _ in range(self.args.num_layers - 1):\n",
    "            self.layers.append(\n",
    "                GraphMatchingConvolution(self.args.dim, self.args.dim, args)\n",
    "            )\n",
    "        self.aggregator = GraphAggregator(self.args.dim, self.args.dim, self.args)\n",
    "        self.layer_outputs = []\n",
    "        self.layer_cross_attentions = []\n",
    "        self.mincut = []\n",
    "        self.mlp = torch.nn.Sequential()\n",
    "        self.args.n_clusters = args.n_clusters\n",
    "        self.mlp.append(Linear(self.args.dim, self.args.n_clusters))\n",
    "        self.topk_outputs = []\n",
    "        self.norms_per_layer = []\n",
    "        self.attention_sums_per_layer = []\n",
    "\n",
    "    def compute_emb(\n",
    "        self, feats, edge_index, batch, sizes_1, sizes_2, edge_index_1, edge_index_2\n",
    "    ):\n",
    "\n",
    "        topk_pooling = TopKPooling(\n",
    "            self.args.dim, ratio=min(sizes_1.item(), sizes_2.item())\n",
    "        )\n",
    "        for i in range(self.args.num_layers):\n",
    "            (\n",
    "                feats,\n",
    "                edge_index,\n",
    "                batch,\n",
    "                (\n",
    "                    attention_input,\n",
    "                    cross_graph_attention,\n",
    "                    a_x,\n",
    "                    a_y,\n",
    "                    norms,\n",
    "                    attention_sums,\n",
    "                ),\n",
    "            ) = self.layers[i](feats, edge_index, batch)\n",
    "\n",
    "            x_1 = feats[: sizes_1.item(), :]\n",
    "            x_2 = feats[sizes_1.item() : sizes_1.item() + sizes_2.item(), :]\n",
    "\n",
    "            norms_1 = norms[: sizes_1.item()]\n",
    "            norms_2 = norms[sizes_1.item() : sizes_1.item() + sizes_2.item()]\n",
    "\n",
    "            attention_sums_1 = attention_sums[: sizes_1.item()]\n",
    "            attention_sums_2 = attention_sums[\n",
    "                sizes_1.item() : sizes_1.item() + sizes_2.item()\n",
    "            ]\n",
    "\n",
    "            x_pooled_1, edge_index_pooled_1, _, _, perm1, score1 = topk_pooling(\n",
    "                x_1,\n",
    "                edge_index_1,\n",
    "            )\n",
    "            x_pooled_2, edge_index_pooled_2, _, _, perm2, score2 = topk_pooling(\n",
    "                x_2,\n",
    "                edge_index_2,\n",
    "            )\n",
    "\n",
    "            self.topk_outputs.append(\n",
    "                (\n",
    "                    (x_pooled_1, edge_index_pooled_1, perm1, score1),\n",
    "                    (x_pooled_2, edge_index_pooled_2, perm2, score2),\n",
    "                )\n",
    "            )\n",
    "            self.layer_cross_attentions.append((cross_graph_attention, a_x, a_y))\n",
    "            self.layer_outputs.append((x_1, edge_index_1, x_2, edge_index_2))\n",
    "            self.norms_per_layer.append((norms_1, norms_2))\n",
    "            self.attention_sums_per_layer.append((attention_sums_1, attention_sums_2))\n",
    "\n",
    "        feats = self.aggregator(feats, edge_index, batch)\n",
    "        return feats, edge_index, batch\n",
    "\n",
    "    def combine_pair_embedding(\n",
    "        self, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "    ):\n",
    "        feats = torch.cat([feats_1, feats_2], dim=0)\n",
    "        max_node_idx_1 = sizes_1.sum()\n",
    "        edge_index_2_offset = edge_index_2 + max_node_idx_1\n",
    "        edge_index = torch.cat([edge_index_1, edge_index_2_offset], dim=1)\n",
    "        batch = create_batch(torch.cat([sizes_1, sizes_2], dim=0))\n",
    "        feats, edge_index, batch = (\n",
    "            feats.to(self.args.device),\n",
    "            edge_index.to(self.args.device),\n",
    "            batch.to(self.args.device),\n",
    "        )\n",
    "        return feats, edge_index, batch\n",
    "\n",
    "    def forward(self, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2):\n",
    "        self.layer_outputs = []\n",
    "        self.layer_cross_attentions = []\n",
    "        self.topk_outputs = []\n",
    "        self.mincut = []\n",
    "        self.norms_per_layer = []\n",
    "        self.attention_sums_per_layer = []\n",
    "\n",
    "        feats, edge_index, batch = self.combine_pair_embedding(\n",
    "            feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "        )\n",
    "        emb, _, _ = self.compute_emb(\n",
    "            feats, edge_index, batch, sizes_1, sizes_2, edge_index_1, edge_index_2\n",
    "        )\n",
    "        emb_1 = emb[: emb.shape[0] // 2, :]\n",
    "        emb_2 = emb[emb.shape[0] // 2 :, :]\n",
    "\n",
    "        best_acc1, best_acc2 = 0.0, 0.0\n",
    "        cluster1, cluster2 = None, None\n",
    "        layer1, layer2 = None, None\n",
    "        for i in range(len(self.topk_outputs)):\n",
    "            (\n",
    "                (x_pooled_1, edge_index_pooled_1, perm1, score1),\n",
    "                (x_pooled_2, edge_index_pooled_2, perm2, score2),\n",
    "            ) = self.topk_outputs[i]\n",
    "            acc1 = len(set(range(8)) & set(perm1.tolist()))\n",
    "            acc2 = len(set(range(8)) & set(perm2.tolist()))\n",
    "            if acc1 > best_acc1:\n",
    "                cluster1 = Data(x=x_pooled_1, edge_index=edge_index_pooled_1)\n",
    "                best_acc1 = acc1\n",
    "                layer1 = i + 1\n",
    "            if acc2 > best_acc2:\n",
    "                cluster2 = Data(x=x_pooled_2, edge_index=edge_index_pooled_2)\n",
    "                best_acc2 = acc2\n",
    "                layer2 = i + 1\n",
    "\n",
    "        return emb_1, emb_2, cluster1, cluster2, layer1, layer2\n",
    "\n",
    "    def compute_metrics(self, emb_1, emb_2, labels):\n",
    "        distances = torch.norm(emb_1 - emb_2, p=2, dim=1)\n",
    "        loss = F.relu(self.margin - labels * (1 - distances)).mean()\n",
    "        predicted_similar = torch.where(\n",
    "            distances < self.args.margin,\n",
    "            torch.ones_like(labels),\n",
    "            -torch.ones_like(labels),\n",
    "        )\n",
    "        acc = (predicted_similar == labels).float().mean()\n",
    "        metrics = {\"loss\": loss, \"acc\": acc}\n",
    "        return metrics\n",
    "\n",
    "    def init_metric_dict(self):\n",
    "        return {\"acc\": -1, \"f1\": -1}\n",
    "\n",
    "    def has_improved(self, m1, m2):\n",
    "        return m1[\"acc\"] < m2[\"acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GraphDataset(torch.load(\"data/cycle_line_star_complete_1.pt\"))\n",
    "small_graphs, medium_graphs, large_graphs, classes = analyze_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, pairs, labels, batch_size):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    losses = []\n",
    "    accs = []\n",
    "\n",
    "    def get_params(model):\n",
    "        return {name: param.clone() for name, param in model.named_parameters()}\n",
    "\n",
    "    initial_params = get_params(model)\n",
    "\n",
    "    for i in range(len(pairs)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        graph1, graph2 = pairs[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "        feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "        sizes_1 = torch.tensor([graph1.num_nodes])\n",
    "        sizes_2 = torch.tensor([graph2.num_nodes])\n",
    "\n",
    "        emb_1, emb_2, _, _, _, _ = model(\n",
    "            feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "        )\n",
    "\n",
    "        metrics = model.compute_metrics(emb_1, emb_2, torch.tensor([label]))\n",
    "        loss = metrics[\"loss\"]\n",
    "        acc = metrics[\"acc\"]\n",
    "\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "\n",
    "        if i % batch_size == 0 and i > 0:\n",
    "            batch_loss = torch.mean(torch.stack(losses))\n",
    "            batch_acc = torch.mean(torch.stack(accs))\n",
    "            losses = []\n",
    "            accs = []\n",
    "            train_losses.append(batch_loss.detach().numpy())\n",
    "            train_accuracies.append(batch_acc.detach().numpy())\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            # if i % 100 * batch_size == 0:\n",
    "            #     print(\n",
    "            #         f\"Epoch: {i} - Loss: {batch_loss.item():.4f}, Acc: {batch_acc:.4f}\"\n",
    "            #     )\n",
    "\n",
    "    trained_params = get_params(model)\n",
    "\n",
    "    # for name in initial_params:\n",
    "    #     initial_param = initial_params[name]\n",
    "    #     trained_param = trained_params[name]\n",
    "    #     if not torch.equal(initial_param, trained_param):\n",
    "    #         print(f\"Parameter {name} has changed.\")\n",
    "    #     else:\n",
    "    #         print(f\"Parameter {name} has NOT changed.\")\n",
    "\n",
    "    # plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.plot(train_losses, label=\"Training Loss\")\n",
    "    # plt.title(\"Loss over Epochs\")\n",
    "    # plt.xlabel(\"Epochs\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "    # plt.legend()\n",
    "\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "    # plt.title(\"Accuracy over Epochs\")\n",
    "    # plt.xlabel(\"Epochs\")\n",
    "    # plt.ylabel(\"Accuracy\")\n",
    "    # plt.legend()\n",
    "\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, title=\"\"):\n",
    "    class_clusters = []\n",
    "    class_accs = []\n",
    "    for i in range(4):\n",
    "        c = f\"class_{str(i)}\"\n",
    "        idx1 = random.sample(range(len(classes[c])), 1)[0]\n",
    "        idx2 = random.sample(range(len(classes[c])), 1)[0]\n",
    "        graph1, graph2 = classes[c][idx1], classes[c][idx2]\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "        feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "        sizes_1 = torch.tensor([len(graph1.x)])\n",
    "        sizes_2 = torch.tensor([len(graph2.x)])\n",
    "        _, _, cluster1, cluster2, _, _ = model(\n",
    "            feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "        )\n",
    "\n",
    "        clusters = []\n",
    "        accs = []\n",
    "\n",
    "        for i in range(len(model.topk_outputs)):\n",
    "            (\n",
    "                (x_pooled_1, edge_index_pooled_1, perm1, score1),\n",
    "                (x_pooled_2, edge_index_pooled_2, perm2, score2),\n",
    "            ) = model.topk_outputs[i]\n",
    "            clusters.append(\n",
    "                (\n",
    "                    Data(x=x_pooled_1, edge_index=edge_index_pooled_1),\n",
    "                    Data(x=x_pooled_2, edge_index=edge_index_pooled_2),\n",
    "                )\n",
    "            )\n",
    "            accs.append(\n",
    "                (\n",
    "                    len(set(range(8)) & set(perm1.tolist())),\n",
    "                    len(set(range(8)) & set(perm2.tolist())),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        acc = list(itertools.chain.from_iterable(zip(*accs)))\n",
    "        class_accs.extend(acc)\n",
    "\n",
    "        cs = list(itertools.chain.from_iterable(zip(*clusters)))\n",
    "        class_clusters.extend(cs)\n",
    "        # plot_graph_pair(cluster1, cluster2)\n",
    "\n",
    "    norm_barplot(model, 0)\n",
    "    norm_barplot(model, 1)\n",
    "    cross_barplot(model, 0)\n",
    "    cross_barplot(model, 1)\n",
    "    plot_all_classes(class_clusters, class_accs, title, model.args.num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "def best_k(model, threshold=0.8):\n",
    "    def calculate_cumulative_scores(scores):\n",
    "        normalized_scores = F.softmax(scores, dim=0)\n",
    "        cumulative_scores = torch.cumsum(normalized_scores, dim=0)\n",
    "        return cumulative_scores\n",
    "\n",
    "    def calculate_confidence(cumulative_scores, k):\n",
    "        gradients = np.gradient(cumulative_scores.detach().numpy())\n",
    "        confidence = gradients[k - 1]\n",
    "        return confidence\n",
    "\n",
    "    def find_best_k_and_confidence(cumulative_scores, total_percentage):\n",
    "        k = ((cumulative_scores / cumulative_scores[-1]) >= total_percentage).nonzero()[\n",
    "            0\n",
    "        ].item() + 1\n",
    "        confidence = calculate_confidence(cumulative_scores, k)\n",
    "        return k, confidence\n",
    "\n",
    "    layer_scores_1 = []\n",
    "    layer_scores_2 = []\n",
    "    for i in range(len(model.topk_outputs)):\n",
    "        (\n",
    "            (_, _, _, score1),\n",
    "            (_, _, _, score2),\n",
    "        ) = model.topk_outputs[i]\n",
    "        layer_scores_1.append(score1)\n",
    "        layer_scores_2.append(score2)\n",
    "\n",
    "    k_values_graph1 = []\n",
    "    confidences_graph1 = []\n",
    "    k_stds1 = []\n",
    "    k_gradients1 = []\n",
    "    for i, scores in enumerate(layer_scores_1):\n",
    "        cumulative_scores = calculate_cumulative_scores(scores)\n",
    "        best_k, confidence = find_best_k_and_confidence(cumulative_scores, threshold)\n",
    "        k_values_graph1.append(best_k)\n",
    "        mean_score = scores.mean()\n",
    "        confidences_graph1.append(confidence)\n",
    "\n",
    "    k_values_graph2 = []\n",
    "    confidences_graph2 = []\n",
    "    k_stds2 = []\n",
    "    k_gradients2 = []\n",
    "    for i, scores in enumerate(layer_scores_2):\n",
    "        cumulative_scores = calculate_cumulative_scores(scores)\n",
    "        best_k, confidence = find_best_k_and_confidence(cumulative_scores, threshold)\n",
    "        k_values_graph2.append(best_k)\n",
    "        confidences_graph2.append(confidence)\n",
    "        std_dev = scores.std()\n",
    "        k = (scores > mean_score - std_dev).sum().item()\n",
    "        k_stds2.append(k)\n",
    "        # gradients = scores.grad\n",
    "        # k_grad = (gradients.abs() > 0.8).sum().item()\n",
    "        # k_gradients2.append(k_grad)\n",
    "\n",
    "    def calculate_weighted_average(ks, confidences):\n",
    "        normalized_confidences = [float(i) / sum(confidences) for i in confidences]\n",
    "        weighted_ks = sum(k * w for k, w in zip(ks, normalized_confidences))\n",
    "        return int(round(weighted_ks))\n",
    "\n",
    "    combined_ks = k_values_graph1 + k_values_graph2\n",
    "    combined_confidences = confidences_graph1 + confidences_graph2\n",
    "\n",
    "    overall_k_confidence = calculate_weighted_average(combined_ks, combined_confidences)\n",
    "    overall_k_median = int(np.median(combined_ks))\n",
    "\n",
    "    overall_k_mode = mode(combined_ks).mode\n",
    "\n",
    "    return (\n",
    "        overall_k_confidence,\n",
    "        overall_k_median,\n",
    "        k_values_graph1,\n",
    "        k_values_graph2,\n",
    "        overall_k_mode,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_graph(graphs):\n",
    "    nx_graphs = [to_networkx(graph, to_undirected=True) for graph in graphs]\n",
    "\n",
    "    def graph_hash(G):\n",
    "        sorted_edges = tuple(sorted((min(edge), max(edge)) for edge in G.edges))\n",
    "        return sorted_edges\n",
    "\n",
    "    hashes = [graph_hash(g) for g in nx_graphs]\n",
    "    frequency = Counter(hashes)\n",
    "    most_common_hash, _ = frequency.most_common(1)[0]\n",
    "    for graph, h in zip(graphs, hashes):\n",
    "        if h == most_common_hash:\n",
    "            return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_confidence(scores, external_weights=None, w_avg=0.5, w_var=0.5):\n",
    "    # Calculate average score\n",
    "    average_score = scores.mean()\n",
    "\n",
    "    # Calculate variance of the scores\n",
    "    variance_score = scores.var()\n",
    "\n",
    "    # If variance is zero (all scores are the same), handle division by zero in inverse variance calculation\n",
    "    if variance_score == 0:\n",
    "        inverse_variance = torch.tensor(0)\n",
    "    else:\n",
    "        inverse_variance = 1 / variance_score\n",
    "\n",
    "    # Combine average score and inverse variance\n",
    "    weighted_confidence = w_avg * average_score + w_var * inverse_variance\n",
    "\n",
    "    # If there are external weights to include\n",
    "    if external_weights is not None:\n",
    "        weighted_confidence += (\n",
    "            external_weights.sum() * 0.1\n",
    "        )  # Assuming external_weights is also a tensor\n",
    "\n",
    "    return weighted_confidence.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_total_ged_graph(graphs):\n",
    "    \"\"\"\n",
    "    Finds the graph that has the smallest total graph edit distance (GED) to all other graphs in the list.\n",
    "\n",
    "    Parameters:\n",
    "        graphs (list of Data): The list of PyTorch Geometric graph data instances.\n",
    "\n",
    "    Returns:\n",
    "        Data: The PyTorch Geometric graph with the smallest total GED.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert PyTorch Geometric graphs to NetworkX graphs\n",
    "    nx_graphs = [to_networkx(graph, to_undirected=True) for graph in graphs]\n",
    "\n",
    "    # Initialize a list to store the total GED for each graph\n",
    "    total_geds = [0] * len(nx_graphs)\n",
    "\n",
    "    # Compute the GED between each pair of graphs\n",
    "    for i, graph_i in enumerate(nx_graphs):\n",
    "        for j, graph_j in enumerate(nx_graphs):\n",
    "            if i != j:\n",
    "                # Initialize a variable to store the minimum GED for the current pair\n",
    "                min_ged = float(\"inf\")\n",
    "                # Iterate over the generator to find the minimal GED\n",
    "                for ged_estimate in nx.optimize_graph_edit_distance(graph_i, graph_j):\n",
    "                    min_ged = min(min_ged, ged_estimate)\n",
    "                # Add the minimal GED to the total GED of the current graph\n",
    "                total_geds[i] += min_ged\n",
    "\n",
    "    # Find the index of the graph with the smallest total GED\n",
    "    min_ged_index = total_geds.index(min(total_geds))\n",
    "\n",
    "    # Return the corresponding PyTorch Geometric graph\n",
    "    return graphs[min_ged_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_cluster(model, k_threshold=0.8):\n",
    "    accs = []\n",
    "    cs = []\n",
    "    clustered_graphs = []\n",
    "\n",
    "    k_confidence, k_median, k_values_graph1, k_values_graph2, k_mode = best_k(\n",
    "        model, k_threshold\n",
    "    )\n",
    "\n",
    "    best_acc1, best_acc2 = 0.0, 0.0\n",
    "    cluster1, cluster2 = None, None\n",
    "    layer1, layer2 = None, None\n",
    "\n",
    "    layers = []\n",
    "    sims = []\n",
    "\n",
    "    for i in range(len(model.layer_outputs)):\n",
    "        (x_1, edge_index_1, x_2, edge_index_2) = model.layer_outputs[i]\n",
    "\n",
    "        topk_pooling = TopKPooling(model.args.dim, ratio=8)\n",
    "\n",
    "        x_pooled_1, edge_index_pooled_1, _, _, perm1, score1 = topk_pooling(\n",
    "            x_1,\n",
    "            edge_index_1,\n",
    "        )\n",
    "        x_pooled_2, edge_index_pooled_2, _, _, perm2, score2 = topk_pooling(\n",
    "            x_2,\n",
    "            edge_index_2,\n",
    "        )\n",
    "\n",
    "        acc1 = len(set(range(8)) & set(perm1.tolist()))\n",
    "        acc2 = len(set(range(8)) & set(perm2.tolist()))\n",
    "\n",
    "        layers.append(\n",
    "            (\n",
    "                (x_pooled_1, edge_index_pooled_1, score1),\n",
    "                (x_pooled_2, edge_index_pooled_2, score2),\n",
    "                i,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        clustered_graphs.append(\n",
    "            (\n",
    "                (\n",
    "                    Data(x=x_pooled_1, edge_index=edge_index_pooled_1),\n",
    "                    # abs(score1).sum().item(),\n",
    "                    calculate_weighted_confidence(score1),\n",
    "                ),\n",
    "                (\n",
    "                    Data(x=x_pooled_2, edge_index=edge_index_pooled_2),\n",
    "                    # abs(score2).sum().item(),\n",
    "                    calculate_weighted_confidence(score2),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if acc1 > best_acc1:\n",
    "            x = x_pooled_1\n",
    "            cluster1 = Data(x=x_pooled_1, edge_index=edge_index_pooled_1)\n",
    "            best_acc1 = acc1\n",
    "            layer1 = i + 1\n",
    "        if acc2 > best_acc2:\n",
    "            cluster2 = Data(x=x_pooled_2, edge_index=edge_index_pooled_2)\n",
    "            best_acc2 = acc2\n",
    "            layer2 = i + 1\n",
    "\n",
    "    cs.append((layers, layer1, layer2))\n",
    "\n",
    "    accs.append(best_acc1)\n",
    "    accs.append(best_acc2)\n",
    "\n",
    "    connected_1 = []\n",
    "    connected_2 = []\n",
    "    all_clusters = []\n",
    "\n",
    "    for i in range(len(clustered_graphs)):\n",
    "        (g1, s1), (g2, s2) = clustered_graphs[i]\n",
    "        if nx.is_connected(to_networkx(g1, to_undirected=True)):\n",
    "            connected_1.append((g1, s1))\n",
    "        if nx.is_connected(to_networkx(g2, to_undirected=True)):\n",
    "            connected_2.append((g2, s2))\n",
    "        all_clusters.append((g1, s1))\n",
    "        all_clusters.append((g2, s2))\n",
    "\n",
    "    graphs = []\n",
    "\n",
    "    for i, (g1, s1) in enumerate(connected_1):\n",
    "        for j, (g2, s2) in enumerate(connected_2):\n",
    "            if nx.is_isomorphic(\n",
    "                to_networkx(g1, to_undirected=True), to_networkx(g2, to_undirected=True)\n",
    "            ):\n",
    "                graphs.append(\n",
    "                    (Data(x=(g1.x + g2.x) / 2, edge_index=g1.edge_index), i + 1, j + 1)\n",
    "                )\n",
    "\n",
    "    # for graph in graphs:\n",
    "    # plot_graph(graph[0])\n",
    "\n",
    "    connected = connected_1 + connected_2\n",
    "    connected = sorted(connected, key=lambda x: x[1])\n",
    "\n",
    "    if graphs != []:\n",
    "        return (\n",
    "            find_most_common_graph([g[0] for g in graphs]),\n",
    "            k_confidence,\n",
    "            cluster1,\n",
    "            cluster2,\n",
    "            k_mode,\n",
    "            \"isomorphic\",\n",
    "        )\n",
    "    elif connected_1 != [] or connected_2 != []:\n",
    "        return (\n",
    "            # random.choice(connected_1 + connected_2)[0],\n",
    "            connected[0][0],\n",
    "            k_confidence,\n",
    "            cluster1,\n",
    "            cluster2,\n",
    "            k_mode,\n",
    "            \"connected\",\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            random.choice(all_clusters)[0],\n",
    "            k_confidence,\n",
    "            cluster1,\n",
    "            cluster2,\n",
    "            k_mode,\n",
    "            \"random\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_test(model, print_results=True, k_threshold=0.8):\n",
    "    correct_class0 = 0\n",
    "    correct_class1 = 0\n",
    "    correct_class2 = 0\n",
    "    correct_class3 = 0\n",
    "    correct_class0_new = 0\n",
    "    correct_class1_new = 0\n",
    "    correct_class2_new = 0\n",
    "    correct_class3_new = 0\n",
    "    best_class0 = 0\n",
    "    best_class1 = 0\n",
    "    best_class2 = 0\n",
    "    best_class3 = 0\n",
    "    layers_class0 = []\n",
    "    layers_class1 = []\n",
    "    layers_class2 = []\n",
    "    layers_class3 = []\n",
    "    ks = []\n",
    "    ks_mode = []\n",
    "    ts = {\"isomorphic\": 0, \"connected\": 0, \"random\": 0}\n",
    "    correct_isomorphic = 0\n",
    "    correct_connected = 0\n",
    "\n",
    "    for _ in range(500):\n",
    "        for i in range(4):\n",
    "            c = f\"class_{str(i)}\"\n",
    "            idx1 = random.sample(range(len(classes[c])), 1)[0]\n",
    "            idx2 = random.sample(range(len(classes[c])), 1)[0]\n",
    "            graph1, graph2 = classes[c][idx1], classes[c][idx2]\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "            feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "            sizes_1 = torch.tensor([len(graph1.x)])\n",
    "            sizes_2 = torch.tensor([len(graph2.x)])\n",
    "            emb1, emb2, cluster1, cluster2, layer1, layer2 = model(\n",
    "                feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "            )\n",
    "\n",
    "            cluster, k, cluster1, cluster2, k_mode, t = select_cluster(\n",
    "                model, k_threshold\n",
    "            )\n",
    "            ks.append(k)\n",
    "            ks_mode.append(k_mode)\n",
    "            ts[t] += 1\n",
    "\n",
    "            if c == \"class_0\":\n",
    "                layers_class0.append(layer1)\n",
    "                layers_class0.append(layer2)\n",
    "                correct_class0 += is_cycle(cluster1) + is_cycle(cluster2)\n",
    "                best_class0 += any([is_cycle(cluster1), is_cycle(cluster2)])\n",
    "                correct_class0_new += is_cycle(cluster)\n",
    "                if t == \"isomorphic\":\n",
    "                    correct_isomorphic += is_cycle(cluster)\n",
    "                elif t == \"connected\":\n",
    "                    correct_connected += is_cycle(cluster)\n",
    "            elif c == \"class_1\":\n",
    "                layers_class1.append(layer1)\n",
    "                layers_class1.append(layer2)\n",
    "                correct_class1 += is_complete(cluster1) + is_complete(cluster2)\n",
    "                best_class1 += any([is_complete(cluster1), is_complete(cluster2)])\n",
    "                correct_class1_new += is_complete(cluster)\n",
    "                if t == \"isomorphic\":\n",
    "                    correct_isomorphic += is_complete(cluster)\n",
    "                elif t == \"connected\":\n",
    "                    correct_connected += is_complete(cluster)\n",
    "            elif c == \"class_2\":\n",
    "                layers_class2.append(layer1)\n",
    "                layers_class2.append(layer2)\n",
    "                correct_class2 += is_line(cluster1) + is_line(cluster2)\n",
    "                best_class2 += any([is_line(cluster1), is_line(cluster2)])\n",
    "                correct_class2_new += is_line(cluster)\n",
    "                if t == \"isomorphic\":\n",
    "                    correct_isomorphic += is_line(cluster)\n",
    "                elif t == \"connected\":\n",
    "                    correct_connected += is_line(cluster)\n",
    "            elif c == \"class_3\":\n",
    "                layers_class3.append(layer1)\n",
    "                layers_class3.append(layer2)\n",
    "                correct_class3 += is_star(cluster1) + is_star(cluster2)\n",
    "                best_class3 += any([is_star(cluster1), is_star(cluster2)])\n",
    "                correct_class3_new += is_star(cluster)\n",
    "                if t == \"isomorphic\":\n",
    "                    correct_isomorphic += is_star(cluster)\n",
    "                elif t == \"connected\":\n",
    "                    correct_connected += is_star(cluster)\n",
    "\n",
    "    class0_acc = correct_class0 / 10\n",
    "    class1_acc = correct_class1 / 10\n",
    "    class2_acc = correct_class2 / 10\n",
    "    class3_acc = correct_class3 / 10\n",
    "    overall_acc = (class0_acc + class1_acc + class2_acc + class3_acc) / 4\n",
    "    class0_acc_new = correct_class0_new / 5\n",
    "    class1_acc_new = correct_class1_new / 5\n",
    "    class2_acc_new = correct_class2_new / 5\n",
    "    class3_acc_new = correct_class3_new / 5\n",
    "    overall_acc_new = (\n",
    "        class0_acc_new + class1_acc_new + class2_acc_new + class3_acc_new\n",
    "    ) / 4\n",
    "    best_class0_acc = best_class0 / 5\n",
    "    best_class1_acc = best_class1 / 5\n",
    "    best_class2_acc = best_class2 / 5\n",
    "    best_class3_acc = best_class3 / 5\n",
    "    best_overall_acc = (\n",
    "        best_class0_acc + best_class1_acc + best_class2_acc + best_class3_acc\n",
    "    ) / 4\n",
    "    # plot_layer_barplot(\n",
    "    #     layers_class0, layers_class1, layers_class2, layers_class3, num_layers\n",
    "    # )\n",
    "    unique, counts = np.unique(ks, return_counts=True)\n",
    "    counts = counts / len(ks)\n",
    "    results = dict(zip(unique, counts))\n",
    "\n",
    "    unique_mode, counts_mode = np.unique(ks_mode, return_counts=True)\n",
    "    counts_mode = counts_mode / len(ks_mode)\n",
    "    results_mode = dict(zip(unique_mode, counts_mode))\n",
    "    if print_results:\n",
    "        print(f\"Correct cycle predictions: {class0_acc:.1f}% ({best_class0_acc:.1f}%)\")\n",
    "        print(\n",
    "            f\"Correct complete predictions: {class1_acc:.1f}% ({best_class1_acc:.1f}%)\"\n",
    "        )\n",
    "        print(f\"Correct line predictions: {class2_acc:.1f}% ({best_class2_acc:.1f}%)\")\n",
    "        print(f\"Correct star predictions: {class3_acc:.1f}% ({best_class3_acc:.1f}%)\")\n",
    "        print(f\"Overall accuracy: {overall_acc:.1f}% ({best_overall_acc:.1f}%)\")\n",
    "        print(\"-\")\n",
    "        print(f\"New correct cycle predictions: {class0_acc_new:.1f}%\")\n",
    "        print(f\"New correct complete predictions: {class1_acc_new:.1f}%\")\n",
    "        print(f\"New correct line predictions: {class2_acc_new:.1f}%\")\n",
    "        print(f\"New correct star predictions: {class3_acc_new:.1f}%\")\n",
    "        print(f\"New overall accuracy: {overall_acc_new:.1f}%\")\n",
    "        print(\"-\")\n",
    "        print(f\"Selected k: {results}\")\n",
    "        print(f\"Selected k mode: {results_mode}\")\n",
    "        print(f\"Types of selected graphs: {ts}\")\n",
    "        print(f\"Correct isomorphic: {correct_isomorphic}\")\n",
    "        print(f\"Correct connected: {correct_connected}\")\n",
    "    return (class0_acc, class1_acc, class2_acc, class3_acc, overall_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewArgs:\n",
    "    def __init__(self, dim, num_layers, margin, lr, batch_size, num_pairs):\n",
    "        self.dim = dim\n",
    "        self.feat_dim = dataset.num_features\n",
    "        self.num_layers = num_layers\n",
    "        self.margin = margin\n",
    "        self.lr = lr\n",
    "        self.n_classes = dataset.num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.n_clusters = 8\n",
    "        self.num_pairs = num_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = [\n",
    "    (32, 7, 0.5, 0.0001, 128, 3000),\n",
    "    (32, 7, 0.2, 0.01, 64, 500),\n",
    "    (32, 5, 0.2, 0.01, 32, 3000),\n",
    "    (32, 8, 0.3, 0.0001, 64, 1000),\n",
    "    (32, 7, 0.5, 0.01, 32, 3000),\n",
    "    (32, 7, 0.4, 0.0001, 32, 500),\n",
    "    (32, 5, 0.1, 0.01, 64, 3000),\n",
    "    (32, 7, 0.4, 0.0001, 128, 3000),\n",
    "    (32, 7, 0.5, 0.01, 128, 1000),\n",
    "    (32, 8, 0.3, 0.0001, 64, 500),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyperparams in top_10:\n",
    "    newargs = NewArgs(*hyperparams)\n",
    "    model = GraphMatchingNetwork(newargs)\n",
    "    optimizer = Adam(model.parameters(), lr=newargs.lr, weight_decay=1e-5)\n",
    "    pairs, labels = create_graph_pairs(dataset, newargs.num_pairs)\n",
    "    train(model, optimizer, pairs, labels, newargs.batch_size)\n",
    "    acc_test(model, True, 0.8)\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_joint_clusters(\n",
    "    clusters_graph1, clusters_graph2, node_features1, node_features2\n",
    "):\n",
    "    # Example: Find clusters with the highest overlap in average features\n",
    "    max_overlap = 0\n",
    "    best_cluster_pair = None\n",
    "    for cluster1 in clusters_graph1:\n",
    "        for cluster2 in clusters_graph2:\n",
    "            # Calculate feature overlap or intersection\n",
    "            overlap = cosine_similarity(\n",
    "                node_features1[cluster1].mean(0, keepdim=True),\n",
    "                node_features2[cluster2].mean(0, keepdim=True),\n",
    "            )\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_cluster_pair = (cluster1, cluster2)\n",
    "    return best_cluster_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "c = f\"class_0\"\n",
    "idx1 = random.sample(range(len(classes[c])), 1)[0]\n",
    "idx2 = random.sample(range(len(classes[c])), 1)[0]\n",
    "graph1, graph2 = classes[c][idx1], classes[c][idx2]\n",
    "\n",
    "feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "sizes_1 = torch.tensor([len(graph1.x)])\n",
    "sizes_2 = torch.tensor([len(graph2.x)])\n",
    "emb1, emb2, cluster1, cluster2, layer1, layer2 = model(\n",
    "    feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    ")\n",
    "\n",
    "ks_confidence = []\n",
    "ks_median = []\n",
    "accs = []\n",
    "cs = []\n",
    "clustered_graphs = []\n",
    "clusters1 = []\n",
    "clusters2 = []\n",
    "\n",
    "(k_confidence, k_median, k_values_graph1, k_values_graph, k_mode) = best_k(model)\n",
    "ks_confidence.append(k_confidence)\n",
    "ks_median.append(k_median)\n",
    "\n",
    "best_acc1, best_acc2 = 0.0, 0.0\n",
    "cluster1, cluster2 = None, None\n",
    "layer1, layer2 = None, None\n",
    "\n",
    "layers = []\n",
    "sims = []\n",
    "\n",
    "for i in range(len(model.layer_outputs)):\n",
    "    (x_1, edge_index_1, x_2, edge_index_2) = model.layer_outputs[i]\n",
    "\n",
    "    topk_pooling = TopKPooling(model.args.dim, ratio=8)\n",
    "\n",
    "    x_pooled_1, edge_index_pooled_1, _, _, perm1, score1 = topk_pooling(\n",
    "        x_1,\n",
    "        edge_index_1,\n",
    "    )\n",
    "    x_pooled_2, edge_index_pooled_2, _, _, perm2, score2 = topk_pooling(\n",
    "        x_2,\n",
    "        edge_index_2,\n",
    "    )\n",
    "\n",
    "    (x_pooled_1_model, edge_index_pooled_1_model, perm1_model, score1_model), (\n",
    "        x_pooled_2_model,\n",
    "        edge_index_pooled_2_model,\n",
    "        perm2_model,\n",
    "        score2_model,\n",
    "    ) = model.topk_outputs[i]\n",
    "\n",
    "    acc1 = len(set(range(8)) & set(perm1.tolist()))\n",
    "    acc2 = len(set(range(8)) & set(perm2.tolist()))\n",
    "\n",
    "    # print(x_pooled_1_model == x_pooled_1)\n",
    "    # print(edge_index_pooled_1_model == edge_index_pooled_1)\n",
    "    # print(x_pooled_2_model == x_pooled_2)\n",
    "    # print(edge_index_pooled_2_model == edge_index_pooled_2)\n",
    "\n",
    "    layers.append(\n",
    "        (\n",
    "            (x_pooled_1, edge_index_pooled_1, score1),\n",
    "            (x_pooled_2, edge_index_pooled_2, score2),\n",
    "            i,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    clustered_graphs.append(\n",
    "        (\n",
    "            (\n",
    "                Data(x=x_pooled_1, edge_index=edge_index_pooled_1),\n",
    "                # abs(score1).sum().item(),\n",
    "                calculate_weighted_confidence(score1),\n",
    "            ),\n",
    "            (\n",
    "                Data(x=x_pooled_2, edge_index=edge_index_pooled_2),\n",
    "                # abs(score2).sum().item(),\n",
    "                calculate_weighted_confidence(score2),\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    clusters1.append(perm1)\n",
    "    clusters2.append(perm2)\n",
    "\n",
    "    if acc1 > best_acc1:\n",
    "        x = x_pooled_1\n",
    "        cluster1 = Data(x=x_pooled_1, edge_index=edge_index_pooled_1)\n",
    "        best_acc1 = acc1\n",
    "        layer1 = i + 1\n",
    "    if acc2 > best_acc2:\n",
    "        cluster2 = Data(x=x_pooled_2, edge_index=edge_index_pooled_2)\n",
    "        best_acc2 = acc2\n",
    "        layer2 = i + 1\n",
    "\n",
    "cs.append((layers, layer1, layer2))\n",
    "\n",
    "accs.append(best_acc1)\n",
    "accs.append(best_acc2)\n",
    "\n",
    "connected_1 = []\n",
    "connected_2 = []\n",
    "\n",
    "for i in range(len(clustered_graphs)):\n",
    "    (g1, s1), (g2, s2) = clustered_graphs[i]\n",
    "    if nx.is_connected(to_networkx(g1, to_undirected=True)):\n",
    "        connected_1.append((g1, s1))\n",
    "    if nx.is_connected(to_networkx(g2, to_undirected=True)):\n",
    "        connected_2.append((g2, s2))\n",
    "\n",
    "graphs = []\n",
    "\n",
    "for i, (g1, s1) in enumerate(connected_1):\n",
    "    for j, (g2, s2) in enumerate(connected_2):\n",
    "        if nx.is_isomorphic(\n",
    "            to_networkx(g1, to_undirected=True), to_networkx(g2, to_undirected=True)\n",
    "        ):\n",
    "            graphs.append(\n",
    "                (Data(x=(g1.x + g2.x) / 2, edge_index=g1.edge_index), i + 1, j + 1)\n",
    "            )\n",
    "\n",
    "# for graph in graphs:\n",
    "#     plot_graph(graph[0], title=f\"({str(graph[1])}, {str(graph[2])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_joint_clusters(clusters1, clusters2, feats_1, feats_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls, l1, l2 = cs[0]\n",
    "# print(f\"Graph 1 layer: {l1}\")\n",
    "# print(f\"Graph 2 layer: {l2}\")\n",
    "# clusters = []\n",
    "# for i, layer in enumerate(ls):\n",
    "#     (x1, e1, s1), (x2, e2, s2), l = layer\n",
    "#     print(x1.shape)\n",
    "#     g1 = to_nx(x1, e1)\n",
    "#     g2 = to_nx(x2, e2)\n",
    "#     clusters.append(g1)\n",
    "#     print(nx.is_connected(g1), nx.is_connected(g2))\n",
    "#     print(nx.is_isomorphic(g1, g2))\n",
    "#     if i > 0:\n",
    "#         print(sims[i])\n",
    "#     # plot_graph_pair(Data(x=x1, edge_index=e1), Data(x=x2, edge_index=e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_confidence, counts_confidence = np.unique(ks_confidence, return_counts=True)\n",
    "# results_confidence = dict(zip(unique_confidence, counts_confidence))\n",
    "\n",
    "# unique_median, counts_median = np.unique(ks_median, return_counts=True)\n",
    "# results_median = dict(zip(unique_median, counts_median))\n",
    "\n",
    "# unique_accs, count_accs = np.unique(accs, return_counts=True)\n",
    "# count_accs = count_accs / len(accs)\n",
    "# results_accs = dict(zip(unique_accs, count_accs))\n",
    "\n",
    "# print(f\"Accs: {results_accs}\")\n",
    "# print(f\"Confidence: {results_confidence}\")\n",
    "# print(f\"Median: {results_median}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_acc1, best_acc2 = 0.0, 0.0\n",
    "# cluster1, cluster2 = None, None\n",
    "# layer1, layer2 = None, None\n",
    "\n",
    "# topk_pooling = TopKPooling(model.args.dim, ratio=8)\n",
    "\n",
    "# for i in range(len(model.topk_outputs)):\n",
    "#     (\n",
    "#         (x_pooled_1, edge_index_pooled_1, perm1, score1),\n",
    "#         (x_pooled_2, edge_index_pooled_2, perm2, score2),\n",
    "#     ) = model.topk_outputs[i]\n",
    "#     acc1 = len(set(range(8)) & set(perm1.tolist()))\n",
    "#     acc2 = len(set(range(8)) & set(perm2.tolist()))\n",
    "#     if acc1 > best_acc1:\n",
    "#         cluster1 = Data(x=x_pooled_1, edge_index=edge_index_pooled_1)\n",
    "#         best_acc1 = acc1\n",
    "#         layer1 = i + 1\n",
    "#     if acc2 > best_acc2:\n",
    "#         cluster2 = Data(x=x_pooled_2, edge_index=edge_index_pooled_2)\n",
    "#         best_acc2 = acc2\n",
    "#         layer2 = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = torch.norm(model.layer_cross_attentions[4][0], p=2, dim=1)\n",
    "print(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_graphs(graph1, graph2, mapping):\n",
    "    x1, edge_index1 = graph1.x, graph1.edge_index\n",
    "    x2, edge_index2 = graph2.x, graph2.edge_index\n",
    "\n",
    "    new_x_size = x1.size(0) + x2.size(0) - len(mapping)\n",
    "    new_x = torch.zeros((new_x_size, x1.size(1)))\n",
    "\n",
    "    new_index_map = {}\n",
    "    current_index = 0\n",
    "\n",
    "    for idx1, idx2 in mapping.items():\n",
    "        new_x[current_index] = (x1[idx1] + x2[idx2]) / 2\n",
    "        new_index_map[idx1] = current_index\n",
    "        new_index_map[x1.size(0) + idx2] = current_index\n",
    "        current_index += 1\n",
    "\n",
    "    for idx1 in range(x1.size(0)):\n",
    "        if idx1 not in mapping:\n",
    "            new_x[current_index] = x1[idx1]\n",
    "            new_index_map[idx1] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "    for idx2 in range(x2.size(0)):\n",
    "        if idx2 not in mapping.values():\n",
    "            new_x[current_index] = x2[idx2]\n",
    "            new_index_map[x1.size(0) + idx2] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "    new_edge_list = []\n",
    "    for edge in edge_index1.t():\n",
    "        new_edge_list.append(\n",
    "            [new_index_map[edge[0].item()], new_index_map[edge[1].item()]]\n",
    "        )\n",
    "    offset = x1.size(0)\n",
    "    for edge in edge_index2.t():\n",
    "        new_edge_list.append(\n",
    "            [\n",
    "                new_index_map[offset + edge[0].item()],\n",
    "                new_index_map[offset + edge[1].item()],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    new_edge_index = torch.tensor(new_edge_list).t().contiguous()\n",
    "\n",
    "    combined_graph = Data(x=new_x, edge_index=new_edge_index)\n",
    "\n",
    "    return combined_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(clustered_graphs)):\n",
    "    (g1, s1), (g2, s2) = clustered_graphs[i]\n",
    "    # (x_1, edge_index_1, x_2, edge_index_2) = model.layer_outputs[i]\n",
    "    # g1 = Data(x=x_1, edge_index=edge_index_1)\n",
    "    # g2 = Data(x=x_2, edge_index=edge_index_2)\n",
    "    # print(g1.num_nodes, g2.num_nodes)\n",
    "    G1 = to_networkx(g1, to_undirected=True)\n",
    "    G2 = to_networkx(g2, to_undirected=True)\n",
    "    ismags = nx.isomorphism.ISMAGS(G1, G2)\n",
    "    largest_common_subgraph = list(ismags.largest_common_subgraph(symmetry=False))\n",
    "    # print(largest_common_subgraph)\n",
    "    plot_graph_pair(g1, g2, \"Graph 1\", \"Graph 2\")\n",
    "    if nx.is_connected(G1) and nx.is_connected(G2):\n",
    "        for i in range(len(largest_common_subgraph)):\n",
    "            combined_data = combine_graphs(g1, g2, largest_common_subgraph[i])\n",
    "\n",
    "            combined_clustered_x, combined_clustered_edge_index, _, _, _, _ = (\n",
    "                topk_pooling(combined_data.x, combined_data.edge_index)\n",
    "            )\n",
    "\n",
    "            combined_clustered = Data(\n",
    "                x=combined_clustered_x, edge_index=combined_clustered_edge_index\n",
    "            )\n",
    "\n",
    "            if nx.is_connected(to_networkx(combined_clustered, to_undirected=True)):\n",
    "                plot_graph_pair(\n",
    "                    combined_data, combined_clustered, \"Combined\", \"Combined-Clustered\"\n",
    "                )\n",
    "    print(\"---------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
