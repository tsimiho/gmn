{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import BatchNorm, MessagePassing, TopKPooling\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_scatter import scatter_mean\n",
    "from collections import defaultdict\n",
    "\n",
    "from custom.args import grey, purple\n",
    "from custom.dataset import GraphDataset, create_dataset\n",
    "from custom.utils import *\n",
    "from networkx.algorithms.centrality import degree_centrality\n",
    "import pickle\n",
    "from itertools import zip_longest\n",
    "from torch_geometric.utils import degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMatchingConvolution(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, args, aggr=\"add\"):\n",
    "        super(GraphMatchingConvolution, self).__init__(aggr=aggr)\n",
    "        self.args = args\n",
    "        self.lin_node = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_message = torch.nn.Linear(out_channels * 2, out_channels)\n",
    "        self.lin_passing = torch.nn.Linear(out_channels + in_channels, out_channels)\n",
    "        self.batch_norm = BatchNorm(out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x_transformed = self.lin_node(x)\n",
    "        return self.propagate(edge_index, x=x_transformed, original_x=x, batch=batch)\n",
    "\n",
    "    def message(self, edge_index_i, x_i, x_j):\n",
    "        x = torch.cat([x_i, x_j], dim=1)\n",
    "        m = self.lin_message(x)\n",
    "        return m\n",
    "\n",
    "    def update(self, aggr_out, edge_index, x, original_x, batch):\n",
    "        n_graphs = torch.unique(batch).shape[0]\n",
    "        cross_graph_attention, a_x, a_y = batch_block_pair_attention(\n",
    "            original_x, batch, n_graphs\n",
    "        )\n",
    "        attention_input = original_x - cross_graph_attention\n",
    "        aggr_out = self.lin_passing(torch.cat([aggr_out, attention_input], dim=1))\n",
    "        aggr_out = self.batch_norm(aggr_out)\n",
    "\n",
    "        norms = torch.norm(aggr_out, p=2, dim=1)\n",
    "        cross_attention_sums = cross_graph_attention.sum(dim=1)\n",
    "\n",
    "        return (\n",
    "            aggr_out,\n",
    "            edge_index,\n",
    "            batch,\n",
    "            (\n",
    "                attention_input,\n",
    "                cross_graph_attention,\n",
    "                a_x,\n",
    "                a_y,\n",
    "                norms,\n",
    "                cross_attention_sums,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class GraphAggregator(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, args):\n",
    "        super(GraphAggregator, self).__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_gate = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_final = torch.nn.Linear(out_channels, out_channels)\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x_states = self.lin(x)\n",
    "        x_gates = torch.nn.functional.softmax(self.lin_gate(x), dim=1)\n",
    "        x_states = x_states * x_gates\n",
    "        x_states = scatter_mean(x_states, batch, dim=0)\n",
    "        x_states = self.lin_final(x_states)\n",
    "        return x_states\n",
    "\n",
    "\n",
    "class GraphMatchingNetwork(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GraphMatchingNetwork, self).__init__()\n",
    "        self.args = args\n",
    "        self.margin = self.args.margin\n",
    "        if args.n_classes > 2:\n",
    "            self.f1_average = \"micro\"\n",
    "        else:\n",
    "            self.f1_average = \"binary\"\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(\n",
    "            GraphMatchingConvolution(self.args.feat_dim, self.args.dim, args)\n",
    "        )\n",
    "        for _ in range(self.args.num_layers - 1):\n",
    "            self.layers.append(\n",
    "                GraphMatchingConvolution(self.args.dim, self.args.dim, args)\n",
    "            )\n",
    "        self.aggregator = GraphAggregator(self.args.dim, self.args.dim, self.args)\n",
    "        self.layer_outputs = []\n",
    "        self.layer_cross_attentions = []\n",
    "        self.mincut = []\n",
    "        self.mlp = torch.nn.Sequential()\n",
    "        self.args.n_clusters = args.n_clusters\n",
    "        self.mlp.append(Linear(self.args.dim, self.args.n_clusters))\n",
    "        self.topk_outputs = []\n",
    "        self.norms_per_layer = []\n",
    "        self.attention_sums_per_layer = []\n",
    "\n",
    "    def compute_emb(\n",
    "        self, feats, edge_index, batch, sizes_1, sizes_2, edge_index_1, edge_index_2\n",
    "    ):\n",
    "\n",
    "        topk_pooling = TopKPooling(\n",
    "            self.args.dim, ratio=min(sizes_1.item(), sizes_2.item())\n",
    "        )\n",
    "        for i in range(self.args.num_layers):\n",
    "            (\n",
    "                feats,\n",
    "                edge_index,\n",
    "                batch,\n",
    "                (\n",
    "                    attention_input,\n",
    "                    cross_graph_attention,\n",
    "                    a_x,\n",
    "                    a_y,\n",
    "                    norms,\n",
    "                    attention_sums,\n",
    "                ),\n",
    "            ) = self.layers[i](feats, edge_index, batch)\n",
    "\n",
    "            x_1 = feats[: sizes_1.item(), :]\n",
    "            x_2 = feats[sizes_1.item() : sizes_1.item() + sizes_2.item(), :]\n",
    "\n",
    "            norms_1 = norms[: sizes_1.item()]\n",
    "            norms_2 = norms[sizes_1.item() : sizes_1.item() + sizes_2.item()]\n",
    "\n",
    "            attention_sums_1 = attention_sums[: sizes_1.item()]\n",
    "            attention_sums_2 = attention_sums[\n",
    "                sizes_1.item() : sizes_1.item() + sizes_2.item()\n",
    "            ]\n",
    "\n",
    "            x_pooled_1, edge_index_pooled_1, _, _, perm1, score1 = topk_pooling(\n",
    "                x_1,\n",
    "                edge_index_1,\n",
    "            )\n",
    "            x_pooled_2, edge_index_pooled_2, _, _, perm2, score2 = topk_pooling(\n",
    "                x_2,\n",
    "                edge_index_2,\n",
    "            )\n",
    "\n",
    "            self.topk_outputs.append(\n",
    "                (\n",
    "                    (x_pooled_1, edge_index_pooled_1, perm1, score1),\n",
    "                    (x_pooled_2, edge_index_pooled_2, perm2, score2),\n",
    "                )\n",
    "            )\n",
    "            self.layer_cross_attentions.append((cross_graph_attention, a_x, a_y))\n",
    "            self.layer_outputs.append((x_1, edge_index_1, x_2, edge_index_2))\n",
    "            self.norms_per_layer.append((norms_1, norms_2))\n",
    "            self.attention_sums_per_layer.append((attention_sums_1, attention_sums_2))\n",
    "\n",
    "        feats = self.aggregator(feats, edge_index, batch)\n",
    "        return feats, edge_index, batch\n",
    "\n",
    "    def combine_pair_embedding(\n",
    "        self, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "    ):\n",
    "        feats = torch.cat([feats_1, feats_2], dim=0)\n",
    "        max_node_idx_1 = sizes_1.sum()\n",
    "        edge_index_2_offset = edge_index_2 + max_node_idx_1\n",
    "        edge_index = torch.cat([edge_index_1, edge_index_2_offset], dim=1)\n",
    "        batch = create_batch(torch.cat([sizes_1, sizes_2], dim=0))\n",
    "        feats, edge_index, batch = (\n",
    "            feats.to(self.args.device),\n",
    "            edge_index.to(self.args.device),\n",
    "            batch.to(self.args.device),\n",
    "        )\n",
    "        return feats, edge_index, batch\n",
    "\n",
    "    def forward(self, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2):\n",
    "        self.layer_outputs = []\n",
    "        self.layer_cross_attentions = []\n",
    "        self.topk_outputs = []\n",
    "        self.mincut = []\n",
    "        self.norms_per_layer = []\n",
    "        self.attention_sums_per_layer = []\n",
    "\n",
    "        feats, edge_index, batch = self.combine_pair_embedding(\n",
    "            feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "        )\n",
    "        emb, _, _ = self.compute_emb(\n",
    "            feats, edge_index, batch, sizes_1, sizes_2, edge_index_1, edge_index_2\n",
    "        )\n",
    "        emb_1 = emb[: emb.shape[0] // 2, :]\n",
    "        emb_2 = emb[emb.shape[0] // 2 :, :]\n",
    "\n",
    "        best_acc1, best_acc2 = 0.0, 0.0\n",
    "        cluster1, cluster2 = None, None\n",
    "        layer1, layer2 = None, None\n",
    "        clusters = []\n",
    "\n",
    "        return emb_1, emb_2, cluster1, cluster2, layer1, layer2\n",
    "\n",
    "    def compute_metrics(self, emb_1, emb_2, labels):\n",
    "        distances = torch.norm(emb_1 - emb_2, p=2, dim=1)\n",
    "        loss = F.relu(self.margin - labels * (1 - distances)).mean()\n",
    "        predicted_similar = torch.where(\n",
    "            distances < self.args.margin,\n",
    "            torch.ones_like(labels),\n",
    "            -torch.ones_like(labels),\n",
    "        )\n",
    "        acc = (predicted_similar == labels).float().mean()\n",
    "        metrics = {\"loss\": loss, \"acc\": acc}\n",
    "        return metrics\n",
    "\n",
    "    def init_metric_dict(self):\n",
    "        return {\"acc\": -1, \"f1\": -1}\n",
    "\n",
    "    def has_improved(self, m1, m2):\n",
    "        return m1[\"acc\"] < m2[\"acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = TUDataset(\n",
    "#     root=\"data\", name=\"MUTAG\", use_node_attr=True, transform=NormalizeFeatures()\n",
    "# )\n",
    "\n",
    "dataset = GraphDataset(torch.load(\"data/cycle_line_star_complete_1_25+.pt\"))\n",
    "\n",
    "\n",
    "(\n",
    "    small_graphs,\n",
    "    medium_graphs,\n",
    "    large_graphs,\n",
    "    classes,\n",
    "    small_classes,\n",
    "    medium_classes,\n",
    "    large_classes,\n",
    ") = analyze_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, pairs, labels, batch_size, title):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    losses = []\n",
    "    accs = []\n",
    "\n",
    "    for i in range(len(pairs)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        graph1, graph2 = pairs[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "        feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "        sizes_1 = torch.tensor([graph1.num_nodes])\n",
    "        sizes_2 = torch.tensor([graph2.num_nodes])\n",
    "\n",
    "        emb_1, emb_2, _, _, _, _ = model(\n",
    "            feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "        )\n",
    "\n",
    "        metrics = model.compute_metrics(emb_1, emb_2, torch.tensor([label]))\n",
    "        loss = metrics[\"loss\"]\n",
    "        acc = metrics[\"acc\"]\n",
    "\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "\n",
    "        if i % batch_size == 0 and i > 0:\n",
    "            batch_loss = torch.mean(torch.stack(losses))\n",
    "            batch_acc = torch.mean(torch.stack(accs))\n",
    "            losses = []\n",
    "            accs = []\n",
    "            train_losses.append(batch_loss.detach().numpy())\n",
    "            train_accuracies.append(batch_acc.detach().numpy())\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "    plt.title(\"Accuracy over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewArgs:\n",
    "    def __init__(self, dim, num_layers, margin, lr, batch_size, num_pairs):\n",
    "        self.dim = dim\n",
    "        self.feat_dim = dataset.num_features\n",
    "        self.num_layers = num_layers\n",
    "        self.margin = margin\n",
    "        self.lr = lr\n",
    "        self.n_classes = dataset.num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.n_clusters = 8\n",
    "        self.num_pairs = num_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = [\n",
    "    (32, 7, 0.2, 0.01, 64, 500),\n",
    "    (32, 5, 0.2, 0.01, 32, 3000),\n",
    "    (32, 7, 0.5, 0.01, 32, 3000),\n",
    "    (32, 5, 0.1, 0.01, 64, 3000),\n",
    "    (32, 7, 0.5, 0.01, 128, 1000),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hyperparams in top_10:\n",
    "#     newargs = NewArgs(*hyperparams)\n",
    "#     model = GraphMatchingNetwork(newargs)\n",
    "#     optimizer = Adam(model.parameters(), lr=newargs.lr, weight_decay=1e-5)\n",
    "#     pairs, labels = create_graph_pairs(dataset, newargs.num_pairs)\n",
    "#     train(model, optimizer, pairs, labels, newargs.batch_size, str(hyperparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_and_attention(\n",
    "    model, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "):\n",
    "    model(feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2)\n",
    "    embeddings = model.layer_outputs\n",
    "    attentions = model.layer_cross_attentions\n",
    "    return embeddings, attentions\n",
    "\n",
    "\n",
    "def extract_dynamic_attention_nodes(attentions, threshold=0.1):\n",
    "    attention_nodes = []\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    for layer_idx, (cross_graph_attention, a_x, a_y) in enumerate(attentions):\n",
    "        dynamic_attention_nodes_1_to_2 = []\n",
    "        dynamic_attention_nodes_2_to_1 = []\n",
    "\n",
    "        a_x = a_x[0]\n",
    "        a_y = a_y[0]\n",
    "\n",
    "        assert a_x.shape[0] == a_y.shape[0]\n",
    "        assert a_x.shape[1] == a_y.shape[1]\n",
    "\n",
    "        # # Apply softmax normalization\n",
    "        # a_x = softmax(a_x)\n",
    "        # a_y = softmax(a_y.T).T\n",
    "\n",
    "        for i, x_attention in enumerate(a_x):\n",
    "            strong_attention_indices = (x_attention > threshold).nonzero(as_tuple=True)[\n",
    "                0\n",
    "            ]\n",
    "            dynamic_attention_nodes_1_to_2.append(strong_attention_indices.tolist())\n",
    "\n",
    "        for j, y_attention in enumerate(a_y.T):\n",
    "            strong_attention_indices = (y_attention > threshold).nonzero(as_tuple=True)[\n",
    "                0\n",
    "            ]\n",
    "            dynamic_attention_nodes_2_to_1.append(strong_attention_indices.tolist())\n",
    "\n",
    "        attention_nodes.append(\n",
    "            (dynamic_attention_nodes_1_to_2, dynamic_attention_nodes_2_to_1)\n",
    "        )\n",
    "\n",
    "    for i in range(len(attention_nodes)):\n",
    "        for j in range(i):\n",
    "            if attention_nodes[i] == attention_nodes[j]:\n",
    "                for k in range(len(attention_nodes[i][0])):\n",
    "                    attention_nodes[i][0][k] = []\n",
    "                for k in range(len(attention_nodes[i][1])):\n",
    "                    attention_nodes[i][1][k] = []\n",
    "                break\n",
    "\n",
    "    return attention_nodes\n",
    "\n",
    "\n",
    "def print_attentions(attention_nodes, d):\n",
    "    n_layers = len(attention_nodes)\n",
    "    max_length = max(\n",
    "        max(len(attn_1_to_2), len(attn_2_to_1))\n",
    "        for attn_1_to_2, attn_2_to_1 in attention_nodes\n",
    "    )\n",
    "\n",
    "    col_width = 30\n",
    "\n",
    "    if d == \"12\":\n",
    "        print(\" Graph 1 to Graph 2:\".ljust(73))\n",
    "    else:\n",
    "        print(\" Graph 2 to Graph 1:\".ljust(73))\n",
    "    print(\"=\" * 130)\n",
    "\n",
    "    for node_idx in range(max_length):\n",
    "        line = []\n",
    "\n",
    "        for layer_idx in range(n_layers):\n",
    "            if d == \"12\":\n",
    "                if node_idx < len(attention_nodes[layer_idx][0]):\n",
    "                    attn_1_to_2 = attention_nodes[layer_idx][0][node_idx]\n",
    "                    attn_1_to_2_str = f\"{node_idx}: {attn_1_to_2}\".ljust(col_width)\n",
    "                else:\n",
    "                    attn_1_to_2_str = \"\".ljust(col_width)\n",
    "\n",
    "                line.append(attn_1_to_2_str)\n",
    "\n",
    "            else:\n",
    "                if node_idx < len(attention_nodes[layer_idx][1]):\n",
    "                    attn_2_to_1 = attention_nodes[layer_idx][1][node_idx]\n",
    "                    attn_2_to_1_str = f\"{node_idx}: {attn_2_to_1}\".ljust(col_width)\n",
    "                else:\n",
    "                    attn_2_to_1_str = \"\".ljust(col_width)\n",
    "\n",
    "                line.append(attn_2_to_1_str)\n",
    "\n",
    "        g1_to_g2_str = \" | \".join(line)\n",
    "\n",
    "        print(f\"{g1_to_g2_str}\")\n",
    "\n",
    "    print(\"=\" * 130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = f\"class_0\"\n",
    "# idx1 = random.sample(range(len(small_classes[c])), 1)[0]\n",
    "# idx2 = random.sample(range(len(small_classes[c])), 1)[0]\n",
    "# graph1, graph2 = small_classes[c][idx1], small_classes[c][idx2]\n",
    "# model.eval()\n",
    "\n",
    "# feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "# feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "# sizes_1 = torch.tensor([len(graph1.x)])\n",
    "# sizes_2 = torch.tensor([len(graph2.x)])\n",
    "# emb1, emb2, cluster1, cluster2, layer1, layer2 = model(\n",
    "#     feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "# )\n",
    "\n",
    "# embeddings, attentions = extract_embeddings_and_attention(\n",
    "#     model, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "# )\n",
    "\n",
    "# attention_nodes = extract_dynamic_attention_nodes(attentions, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(graph1, graph2, attention_nodes) = torch.load(\"info.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_pairs(attention_nodes, i=0):\n",
    "    outer_layer = attention_nodes[i]\n",
    "    g1_attention, g2_attention = outer_layer\n",
    "\n",
    "    mutual_pairs = []\n",
    "\n",
    "    for g1_node, g1_attends in enumerate(g1_attention):\n",
    "        for g2_node in g1_attends:\n",
    "            if g1_node in g2_attention[g2_node]:\n",
    "                pair = (g1_node, g2_node)\n",
    "                if pair not in mutual_pairs:\n",
    "                    mutual_pairs.append(pair)\n",
    "\n",
    "    random.shuffle(mutual_pairs)\n",
    "    return mutual_pairs\n",
    "\n",
    "\n",
    "# pairs = mutual_pairs(attention_nodes, 1)\n",
    "# pairs.sort()\n",
    "# reversed_pairs = [(p[1], p[0]) for p in pairs]\n",
    "# reversed_pairs.sort()\n",
    "# print(reversed_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def explore(graph1, graph2, attention_nodes, to_print=True):\n",
    "#     explored = set()\n",
    "#     patterns = set()\n",
    "\n",
    "#     def find_all_mutual_pairs(g1_frontier, g2_frontier, mp, explored):\n",
    "#         g1_frontier_list = list(g1_frontier)\n",
    "#         random.shuffle(g1_frontier_list)\n",
    "\n",
    "#         g2_frontier_list = list(g2_frontier)\n",
    "#         random.shuffle(g2_frontier_list)\n",
    "#         mutual_pairs_set = set(mp)\n",
    "#         pairs = []\n",
    "\n",
    "#         for node_g1 in g1_frontier_list:\n",
    "#             for node_g2 in g2_frontier_list:\n",
    "#                 if (node_g1, node_g2) in mutual_pairs_set:\n",
    "#                     if not (node_g1, node_g2) in explored:\n",
    "#                         pairs.append((node_g1, node_g2))\n",
    "\n",
    "#         return pairs\n",
    "\n",
    "#     def step(n1, n2, g1_frontier, g2_frontier, g1_explored, g2_explored, pattern, mp):\n",
    "#         for i in find_neighbors(graph1, n1):\n",
    "#             if not i in g1_explored:\n",
    "#                 g1_frontier.add(i)\n",
    "\n",
    "#         for i in find_neighbors(graph2, n2):\n",
    "#             if not i in g2_explored:\n",
    "#                 g2_frontier.add(i)\n",
    "\n",
    "#         # print(g1_frontier, g2_frontier)\n",
    "#         all_mp = find_all_mutual_pairs(g1_frontier, g2_frontier, mp, explored)\n",
    "#         pattern.add((n1, n2))\n",
    "#         # print(all_mp)\n",
    "\n",
    "#         if all_mp == [] or (\n",
    "#             len(g1_frontier) == len(graph1.x) or len(g2_frontier) == len(graph2.x)\n",
    "#         ):\n",
    "#             if len(pattern) > 2:\n",
    "#                 patterns.add(frozenset(pattern))\n",
    "#             # print(\"---------\")\n",
    "#             return\n",
    "#         else:\n",
    "#             for p in all_mp:\n",
    "#                 n1, n2 = p\n",
    "#                 # print(g1_frontier, g2_frontier)\n",
    "#                 # print(p)\n",
    "#                 # print(\"---------\")\n",
    "\n",
    "#                 g1_frontier_copy = g1_frontier.copy()\n",
    "#                 g2_frontier_copy = g2_frontier.copy()\n",
    "#                 g1_explored_copy = g1_explored.copy()\n",
    "#                 g2_explored_copy = g2_explored.copy()\n",
    "\n",
    "#                 g1_frontier_copy.remove(n1)\n",
    "#                 g2_frontier_copy.remove(n2)\n",
    "#                 g1_explored_copy.add(n1)\n",
    "#                 g2_explored_copy.add(n2)\n",
    "#                 step(\n",
    "#                     n1,\n",
    "#                     n2,\n",
    "#                     g1_frontier_copy.copy(),\n",
    "#                     g2_frontier_copy.copy(),\n",
    "#                     g1_explored_copy.copy(),\n",
    "#                     g2_explored_copy.copy(),\n",
    "#                     pattern.copy(),\n",
    "#                     mp,\n",
    "#                 )\n",
    "\n",
    "#     def find_neighbors(graph, node):\n",
    "#         neighbors = graph.edge_index[1, graph.edge_index[0] == node].tolist()\n",
    "#         neighbors += graph.edge_index[0, graph.edge_index[1] == node].tolist()\n",
    "#         neighbors = list(set(neighbors))\n",
    "#         return neighbors\n",
    "\n",
    "#     for k in range(len(attention_nodes)):\n",
    "\n",
    "#         # print(f\"Layer {k}\")\n",
    "#         # print(\"======================\")\n",
    "\n",
    "#         pairs = mutual_pairs(attention_nodes, k)\n",
    "\n",
    "#         for pair in pairs:\n",
    "#             if pair in explored:\n",
    "#                 continue\n",
    "\n",
    "#             n1, n2 = pair\n",
    "\n",
    "#             # print(f\"Starting with: {pair}\")\n",
    "#             # print(\"---\")\n",
    "\n",
    "#             g1_explored, g2_explored = set(), set()\n",
    "#             g1_frontier, g2_frontier = set(), set()\n",
    "#             g1_explored.add(n1)\n",
    "#             g2_explored.add(n2)\n",
    "#             pattern = set()\n",
    "\n",
    "#             step(\n",
    "#                 n1,\n",
    "#                 n2,\n",
    "#                 g1_frontier.copy(),\n",
    "#                 g2_frontier.copy(),\n",
    "#                 g1_explored.copy(),\n",
    "#                 g2_explored.copy(),\n",
    "#                 pattern.copy(),\n",
    "#                 pairs,\n",
    "#             )\n",
    "\n",
    "#         for pat in patterns:\n",
    "#             for pair_in_pattern in pat:\n",
    "#                 explored.add(pair_in_pattern)\n",
    "\n",
    "#     return patterns\n",
    "\n",
    "\n",
    "# patterns = explore(graph1, graph2, attention_nodes, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subgraphs(pattern, graph1, graph2):\n",
    "    g1_nodes = set()\n",
    "    g2_nodes = set()\n",
    "    for g1_node, g2_node in pattern.items():\n",
    "        g1_nodes.add(g1_node)\n",
    "        g2_nodes.add(g2_node)\n",
    "\n",
    "    g1_node_map = {node: idx for idx, node in enumerate(g1_nodes)}\n",
    "    g2_node_map = {node: idx for idx, node in enumerate(g2_nodes)}\n",
    "\n",
    "    g1_edge_index = []\n",
    "    for edge in graph1.edge_index.t():\n",
    "        if edge[0].item() in g1_nodes and edge[1].item() in g1_nodes:\n",
    "            g1_edge_index.append(\n",
    "                [g1_node_map[edge[0].item()], g1_node_map[edge[1].item()]]\n",
    "            )\n",
    "\n",
    "    g2_edge_index = []\n",
    "    for edge in graph2.edge_index.t():\n",
    "        if edge[0].item() in g2_nodes and edge[1].item() in g2_nodes:\n",
    "            g2_edge_index.append(\n",
    "                [g2_node_map[edge[0].item()], g2_node_map[edge[1].item()]]\n",
    "            )\n",
    "\n",
    "    g1_subgraph = Data(\n",
    "        x=graph1.x[list(g1_nodes)],\n",
    "        edge_index=torch.tensor(g1_edge_index, dtype=torch.long).t().contiguous(),\n",
    "        original_node_ids=torch.tensor(list(g1_nodes), dtype=torch.long),\n",
    "    )\n",
    "    g2_subgraph = Data(\n",
    "        x=graph2.x[list(g2_nodes)],\n",
    "        edge_index=torch.tensor(g2_edge_index, dtype=torch.long).t().contiguous(),\n",
    "        original_node_ids=torch.tensor(list(g2_nodes), dtype=torch.long),\n",
    "    )\n",
    "\n",
    "    return g1_subgraph, g2_subgraph\n",
    "\n",
    "\n",
    "def mutual_pairs(attention_nodes, i=0):\n",
    "    outer_layer = attention_nodes[i]\n",
    "    g1_attention, g2_attention = outer_layer\n",
    "\n",
    "    mutual_pairs = []\n",
    "\n",
    "    for g1_node, g1_attends in enumerate(g1_attention):\n",
    "        for g2_node in g1_attends:\n",
    "            if g1_node in g2_attention[g2_node]:\n",
    "                pair = (g1_node, g2_node)\n",
    "                if pair not in mutual_pairs:\n",
    "                    mutual_pairs.append(pair)\n",
    "\n",
    "    random.shuffle(mutual_pairs)\n",
    "    return mutual_pairs\n",
    "\n",
    "\n",
    "def print_patterns(patterns, graph1, graph2):\n",
    "    for i in patterns:\n",
    "        g1_subgraph, g2_subgraph = create_subgraphs(i, graph1, graph2)\n",
    "\n",
    "        if nx.is_isomorphic(\n",
    "            to_networkx(g1_subgraph, to_undirected=True),\n",
    "            to_networkx(g2_subgraph, to_undirected=True),\n",
    "        ):\n",
    "\n",
    "            plot_mutag(\n",
    "                g1_subgraph,\n",
    "                g2_subgraph,\n",
    "                perm1=g1_subgraph.original_node_ids,\n",
    "                perm2=g2_subgraph.original_node_ids,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import degree\n",
    "import time\n",
    "\n",
    "\n",
    "class MCS:\n",
    "    def __init__(self, mp):\n",
    "        self.mp = mp\n",
    "        self.max_size = 0\n",
    "        self.all_mappings = []\n",
    "        self.unique_mappings = set()\n",
    "        self.visited = set()\n",
    "        self.time_limit = 15\n",
    "        self.start_time = None\n",
    "\n",
    "    def find_mcs(self, G1, G2):\n",
    "        self.max_size = 0\n",
    "        self.all_mappings = []\n",
    "        self.unique_mappings = set()\n",
    "        self.visited = set()\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        G1_degrees = degree(G1.edge_index[0], G1.num_nodes)\n",
    "        G2_degrees = degree(G2.edge_index[0], G2.num_nodes)\n",
    "\n",
    "        nodes1 = list(range(G1.num_nodes))\n",
    "        nodes2 = list(range(G2.num_nodes))\n",
    "\n",
    "        for n1 in nodes1:\n",
    "            for n2 in nodes2:\n",
    "                if (n1, n2) in self.mp:\n",
    "                    M = {n1: n2}\n",
    "                    neighbors1 = set(G1.edge_index[1][G1.edge_index[0] == n1].tolist())\n",
    "                    neighbors2 = set(G2.edge_index[1][G2.edge_index[0] == n2].tolist())\n",
    "                    self.match(\n",
    "                        G1, G2, M, G1_degrees, G2_degrees, neighbors1, neighbors2\n",
    "                    )\n",
    "        return self.all_mappings\n",
    "\n",
    "    def match(self, G1, G2, M, G1_degrees, G2_degrees, neighbors1, neighbors2):\n",
    "        if time.time() - self.start_time > self.time_limit:\n",
    "            return\n",
    "\n",
    "        state = (frozenset(M.items()), frozenset(neighbors1), frozenset(neighbors2))\n",
    "        if state in self.visited:\n",
    "            return\n",
    "        self.visited.add(state)\n",
    "\n",
    "        if len(M) > self.max_size:\n",
    "            self.max_size = len(M)\n",
    "            self.all_mappings = [M.copy()]\n",
    "            self.unique_mappings = {self.canonical_form(M)}\n",
    "        # elif len(M) == self.max_size:\n",
    "        #     canonical = self.canonical_form(M)\n",
    "        #     if canonical not in self.unique_mappings:\n",
    "        #         self.all_mappings.append(M.copy())\n",
    "        #         self.unique_mappings.add(canonical)\n",
    "\n",
    "        candidates1 = sorted(neighbors1, key=lambda n: -G1_degrees[n].item())\n",
    "        candidates2 = sorted(neighbors2, key=lambda n: -G2_degrees[n].item())\n",
    "\n",
    "        for n1 in candidates1:\n",
    "            if n1 not in M:\n",
    "                for n2 in candidates2:\n",
    "                    if n2 not in M.values() and self.feasible(n1, n2, M, G1, G2):\n",
    "                        M[n1] = n2\n",
    "                        new_neighbors1 = set(\n",
    "                            G1.edge_index[1][G1.edge_index[0] == n1].tolist()\n",
    "                        )\n",
    "                        new_neighbors2 = set(\n",
    "                            G2.edge_index[1][G2.edge_index[0] == n2].tolist()\n",
    "                        )\n",
    "                        neighbors1.update(new_neighbors1 - set(M.keys()))\n",
    "                        neighbors2.update(new_neighbors2 - set(M.values()))\n",
    "                        self.match(\n",
    "                            G1,\n",
    "                            G2,\n",
    "                            M,\n",
    "                            G1_degrees,\n",
    "                            G2_degrees,\n",
    "                            neighbors1,\n",
    "                            neighbors2,\n",
    "                        )\n",
    "                        del M[n1]\n",
    "                        neighbors1.difference_update(new_neighbors1)\n",
    "                        neighbors2.difference_update(new_neighbors2)\n",
    "\n",
    "    def feasible(self, n1, n2, M, G1, G2):\n",
    "        if not torch.equal(G1.x[n1], G2.x[n2]):\n",
    "            return False\n",
    "        if (n1, n2) not in self.mp:\n",
    "            return False\n",
    "\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "\n",
    "        for neighbor in G1.edge_index[1][G1.edge_index[0] == n1]:\n",
    "            if neighbor.item() in M:\n",
    "                count1 += 1\n",
    "\n",
    "        for neighbor in G2.edge_index[1][G2.edge_index[0] == n2]:\n",
    "            if neighbor.item() in M.values():\n",
    "                count2 += 1\n",
    "\n",
    "        if count1 != count2:\n",
    "            return False\n",
    "\n",
    "        for neighbor in G1.edge_index[1][G1.edge_index[0] == n1]:\n",
    "            if (\n",
    "                neighbor.item() in M\n",
    "                and not (\n",
    "                    G2.edge_index[1][G2.edge_index[0] == n2] == M[neighbor.item()]\n",
    "                ).any()\n",
    "            ):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def canonical_form(self, M):\n",
    "        G1_set = set(M.keys())\n",
    "        G2_set = set(M.values())\n",
    "        return (frozenset(G1_set), frozenset(G2_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = (32, 7, 0.2, 0.01, 64, 500)\n",
    "newargs = NewArgs(*hyperparams)\n",
    "model = GraphMatchingNetwork(newargs)\n",
    "optimizer = Adam(model.parameters(), lr=newargs.lr, weight_decay=1e-5)\n",
    "pairs, labels = create_graph_pairs(dataset, newargs.num_pairs)\n",
    "train(model, optimizer, pairs, labels, newargs.batch_size, str(hyperparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct_class0 = 0\n",
    "correct_class1 = 0\n",
    "correct_class2 = 0\n",
    "correct_class3 = 0\n",
    "correct_class0_exact = 0\n",
    "correct_class1_exact = 0\n",
    "correct_class2_exact = 0\n",
    "correct_class3_exact = 0\n",
    "ts = []\n",
    "for cn in range(4):\n",
    "    for j in range(25):\n",
    "        c = f\"class_{cn}\"\n",
    "        print(f\"Class {cn} - Pair {j}\")\n",
    "        idx1 = random.sample(range(len(classes[c])), 1)[0]\n",
    "        idx2 = random.sample(range(len(classes[c])), 1)[0]\n",
    "        graph1, graph2 = classes[c][idx1], classes[c][idx2]\n",
    "        model.eval()\n",
    "\n",
    "        feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "        feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "        sizes_1 = torch.tensor([len(graph1.x)])\n",
    "        sizes_2 = torch.tensor([len(graph2.x)])\n",
    "        emb1, emb2, cluster1, cluster2, layer1, layer2 = model(\n",
    "            feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "        )\n",
    "\n",
    "        embeddings, attentions = extract_embeddings_and_attention(\n",
    "            model, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "        )\n",
    "\n",
    "        br = False\n",
    "        for i in range(7):\n",
    "            # for t in [0.05, 0.08, 0.09, 0.1, 0.11, 0.12]:\n",
    "            for t in [0.02, 0.03, 0.04, 0.05]:\n",
    "                attention_nodes = extract_dynamic_attention_nodes(\n",
    "                    attentions, threshold=t\n",
    "                )\n",
    "                mp = mutual_pairs(attention_nodes, i)\n",
    "                vf2 = MCS(mp)\n",
    "                pattern = vf2.find_mcs(graph1, graph2)\n",
    "                if pattern != [] and len(pattern[0]) >= 23:\n",
    "                    g1_subgraph, g2_subgraph = create_subgraphs(\n",
    "                        pattern[0], graph1, graph2\n",
    "                    )\n",
    "\n",
    "                    if (\n",
    "                        nx.is_isomorphic(\n",
    "                            to_networkx(g1_subgraph, to_undirected=True),\n",
    "                            to_networkx(g2_subgraph, to_undirected=True),\n",
    "                        )\n",
    "                        and len(pattern[0]) > 2\n",
    "                    ):\n",
    "                        summary = Data(\n",
    "                            x=(g1_subgraph.x + g2_subgraph.x) / 2,\n",
    "                            edge_index=g1_subgraph.edge_index,\n",
    "                        )\n",
    "\n",
    "                        # plot_graph(summary)\n",
    "\n",
    "                        if c == \"class_0\":\n",
    "                            correct_class0 += is_cycle(summary)\n",
    "                            if is_cycle(summary):\n",
    "                                if len(pattern[0]) == 25:\n",
    "                                    correct_class0_exact += 1\n",
    "                                br = True\n",
    "                                ts.append(t)\n",
    "                                break\n",
    "                        elif c == \"class_1\":\n",
    "                            correct_class1 += is_complete(summary)\n",
    "                            if is_complete(summary):\n",
    "                                if len(pattern[0]) == 25:\n",
    "                                    correct_class1_exact += 1\n",
    "                                ts.append(t)\n",
    "                                br = True\n",
    "                                break\n",
    "                        elif c == \"class_2\":\n",
    "                            correct_class2 += is_line(summary)\n",
    "                            if is_line(summary):\n",
    "                                if len(pattern[0]) == 25:\n",
    "                                    correct_class2_exact += 1\n",
    "                                br = True\n",
    "                                ts.append(t)\n",
    "                                break\n",
    "                        elif c == \"class_3\":\n",
    "                            correct_class3 += is_star(summary)\n",
    "                            if is_star(summary):\n",
    "                                if len(pattern[0]) == 25:\n",
    "                                    correct_class3_exact += 1\n",
    "                                br = True\n",
    "                                ts.append(t)\n",
    "                                break\n",
    "\n",
    "            if br:\n",
    "                break\n",
    "\n",
    "    print(\n",
    "        f\"{correct_class0} ({correct_class0_exact})\",\n",
    "        f\"{correct_class1} ({correct_class1_exact})\",\n",
    "        f\"{correct_class2} ({correct_class2_exact})\",\n",
    "        f\"{correct_class3} ({correct_class3_exact})\",\n",
    "        f\"{correct_class0 + correct_class1 + correct_class2 + correct_class3} ({correct_class0_exact + correct_class1_exact + correct_class2_exact + correct_class3_exact})\",\n",
    "    )\n",
    "    print(Counter(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = f\"class_{cn}\"\n",
    "print(f\"Class {cn} - Pair {j}\")\n",
    "idx1 = random.sample(range(len(classes[c])), 1)[0]\n",
    "idx2 = random.sample(range(len(classes[c])), 1)[0]\n",
    "graph1, graph2 = classes[c][idx1], classes[c][idx2]\n",
    "model.eval()\n",
    "\n",
    "feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "sizes_1 = torch.tensor([len(graph1.x)])\n",
    "sizes_2 = torch.tensor([len(graph2.x)])\n",
    "emb1, emb2, cluster1, cluster2, layer1, layer2 = model(\n",
    "    feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    ")\n",
    "\n",
    "embeddings, attentions = extract_embeddings_and_attention(\n",
    "    model, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    ")\n",
    "\n",
    "br = False\n",
    "for i in range(7):\n",
    "    for t in [0.02]:\n",
    "        attention_nodes = extract_dynamic_attention_nodes(attentions, threshold=t)\n",
    "        mp = mutual_pairs(attention_nodes, i)\n",
    "        vf2 = MCS(mp)\n",
    "        pattern = vf2.find_mcs(graph1, graph2)\n",
    "        if pattern != [] and len(pattern[0]) >= 23:\n",
    "            g1_subgraph, g2_subgraph = create_subgraphs(pattern[0], graph1, graph2)\n",
    "\n",
    "            if (\n",
    "                nx.is_isomorphic(\n",
    "                    to_networkx(g1_subgraph, to_undirected=True),\n",
    "                    to_networkx(g2_subgraph, to_undirected=True),\n",
    "                )\n",
    "                and len(pattern[0]) > 2\n",
    "            ):\n",
    "                summary = Data(\n",
    "                    x=(g1_subgraph.x + g2_subgraph.x) / 2,\n",
    "                    edge_index=g1_subgraph.edge_index,\n",
    "                )\n",
    "\n",
    "                plot_graph(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "\n",
    "# 7or8: 21 20 20 20 81\n",
    "\n",
    "# 8:    18 18 18 13 67\n",
    "\n",
    "# ---------------\n",
    "\n",
    "# 15\n",
    "\n",
    "# 15: 15 20 17 9 61\n",
    "\n",
    "# ---------------\n",
    "\n",
    "# 25\n",
    "\n",
    "# 25: 17 16 20 9 62"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
