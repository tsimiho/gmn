{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "from dataset import GraphDataset\n",
    "from models import *\n",
    "from sklearn.metrics import normalized_mutual_info_score as NMI\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Linear\n",
    "from torch_geometric import utils\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import Planetoid, TUDataset\n",
    "from torch_geometric.nn import DMoNPooling, GCNConv, GraphConv, Sequential, TopKPooling\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GraphDataset(torch.load(\"../data/cycle_line_star_complete_1.pt\"))\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling\n",
    "\n",
    "\n",
    "class TopKPool(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mp_units,\n",
    "        mp_act,\n",
    "        in_channels,\n",
    "        n_clusters,\n",
    "        mlp_units=[],\n",
    "        mlp_act=\"Identity\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.name = \"MinCut\"\n",
    "\n",
    "        mp_act = getattr(torch.nn, mp_act)(inplace=True)\n",
    "        mlp_act = getattr(torch.nn, mlp_act)(inplace=True)\n",
    "\n",
    "        mp = [\n",
    "            (\n",
    "                GCNConv(in_channels, mp_units[0], normalize=False, cached=False),\n",
    "                \"x, edge_index -> x\",\n",
    "            ),\n",
    "            mp_act,\n",
    "        ]\n",
    "        for i in range(len(mp_units) - 1):\n",
    "            mp.append(\n",
    "                (\n",
    "                    GCNConv(\n",
    "                        mp_units[i], mp_units[i + 1], normalize=False, cached=False\n",
    "                    ),\n",
    "                    \"x, edge_index -> x\",\n",
    "                )\n",
    "            )\n",
    "            mp.append(mp_act)\n",
    "        self.mp = Sequential(\"x, edge_index\", mp)\n",
    "        out_chan = mp_units[-1]\n",
    "\n",
    "        self.mlp = torch.nn.Sequential()\n",
    "        for units in mlp_units:\n",
    "            self.mlp.append(Linear(out_chan, units))\n",
    "            out_chan = units\n",
    "            self.mlp.append(mlp_act)\n",
    "        self.mlp.append(Linear(out_chan, n_clusters))\n",
    "\n",
    "        self.pool = TopKPooling(mp_units[-1], ratio=8)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.mp(x, edge_index)\n",
    "        s = self.mlp(x)\n",
    "        adj = utils.to_dense_adj(edge_index)\n",
    "        x_pooled, edge_index_pooled, edge_attr, _, perm, score = self.pool(\n",
    "            x, edge_index, None, batch\n",
    "        )\n",
    "\n",
    "        return torch.softmax(s, dim=-1), perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (642) to match target batch_size (11).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m):\n\u001b[0;32m---> 53\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test(test_loader)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# target = target[perm]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m target \u001b[38;5;241m=\u001b[39m target \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 28\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (642) to match target batch_size (11)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TopKPool(\n",
    "    [64] * 5,\n",
    "    \"ReLU\",\n",
    "    dataset.num_features,\n",
    "    len(np.unique(dataset[0].node_classes)),\n",
    "    [16],\n",
    "    \"ReLU\",\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, perm = model(data)\n",
    "\n",
    "        target = [cls[0] for cls in data.node_classes[0]]\n",
    "        target = torch.tensor(target, device=device, dtype=torch.long)\n",
    "        # target = target[perm]\n",
    "        target = target - 1\n",
    "        loss = F.cross_entropy(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out, perm = model(data)\n",
    "        target_indices = set(range(8))\n",
    "        selected_indices = set(perm.cpu().numpy())\n",
    "        correct += len(target_indices.intersection(selected_indices))\n",
    "        total += len(target_indices)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Correct: {correct}, Total: {total}, Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    train_loss = train()\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {train_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
