{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import BatchNorm, MessagePassing, TopKPooling\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.utils import (\n",
    "    dense_to_sparse,\n",
    "    is_undirected,\n",
    "    to_networkx,\n",
    "    to_undirected,\n",
    ")\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "from custom.args import grey, purple\n",
    "from custom.dataset import GraphDataset, create_dataset\n",
    "from custom.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMatchingConvolution(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, args, aggr=\"add\"):\n",
    "        super(GraphMatchingConvolution, self).__init__(aggr=aggr)\n",
    "        self.args = args\n",
    "        self.lin_node = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_message = torch.nn.Linear(out_channels * 2, out_channels)\n",
    "        self.lin_passing = torch.nn.Linear(out_channels + in_channels, out_channels)\n",
    "        self.batch_norm = BatchNorm(out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x_transformed = self.lin_node(x)\n",
    "        return self.propagate(edge_index, x=x_transformed, original_x=x, batch=batch)\n",
    "\n",
    "    def message(self, edge_index_i, x_i, x_j):\n",
    "        x = torch.cat([x_i, x_j], dim=1)\n",
    "        m = self.lin_message(x)\n",
    "        return m\n",
    "\n",
    "    def update(self, aggr_out, edge_index, x, original_x, batch):\n",
    "        n_graphs = torch.unique(batch).shape[0]\n",
    "        cross_graph_attention, a_x, a_y = batch_block_pair_attention(\n",
    "            original_x, batch, n_graphs\n",
    "        )\n",
    "        attention_input = original_x - cross_graph_attention\n",
    "        aggr_out = self.lin_passing(torch.cat([aggr_out, attention_input], dim=1))\n",
    "        aggr_out = self.batch_norm(aggr_out)\n",
    "        return (\n",
    "            aggr_out,\n",
    "            edge_index,\n",
    "            batch,\n",
    "            (attention_input, cross_graph_attention, a_x, a_y),\n",
    "        )\n",
    "\n",
    "\n",
    "class GraphAggregator(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, args):\n",
    "        super(GraphAggregator, self).__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_gate = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_final = torch.nn.Linear(out_channels, out_channels)\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x_states = self.lin(x)\n",
    "        x_gates = torch.nn.functional.softmax(self.lin_gate(x), dim=1)\n",
    "        x_states = x_states * x_gates\n",
    "        x_states = scatter_mean(x_states, batch, dim=0)\n",
    "        x_states = self.lin_final(x_states)\n",
    "        return x_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMatchingNetwork(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GraphMatchingNetwork, self).__init__()\n",
    "        self.args = args\n",
    "        self.margin = self.args.margin\n",
    "        if args.n_classes > 2:\n",
    "            self.f1_average = \"micro\"\n",
    "        else:\n",
    "            self.f1_average = \"binary\"\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(\n",
    "            GraphMatchingConvolution(self.args.feat_dim, self.args.dim, args)\n",
    "        )\n",
    "        for _ in range(self.args.num_layers - 1):\n",
    "            self.layers.append(\n",
    "                GraphMatchingConvolution(self.args.dim, self.args.dim, args)\n",
    "            )\n",
    "        self.aggregator = GraphAggregator(self.args.dim, self.args.dim, self.args)\n",
    "        self.layer_outputs = []\n",
    "        self.layer_cross_attentions = []\n",
    "        self.mincut = []\n",
    "        self.mlp = torch.nn.Sequential()\n",
    "        self.args.n_clusters = args.n_clusters\n",
    "        self.mlp.append(Linear(self.args.dim, self.args.n_clusters))\n",
    "        self.topk_pooling = TopKPooling(self.args.dim, ratio=8)\n",
    "        self.topk_outputs = []\n",
    "\n",
    "    def compute_emb(\n",
    "        self, feats, edge_index, batch, sizes_1, sizes_2, edge_index_1, edge_index_2\n",
    "    ):\n",
    "        for i in range(self.args.num_layers):\n",
    "            (\n",
    "                feats,\n",
    "                edge_index,\n",
    "                batch,\n",
    "                (attention_input, cross_graph_attention, a_x, a_y),\n",
    "            ) = self.layers[i](feats, edge_index, batch)\n",
    "            x_1 = feats[: sizes_1.item(), :]\n",
    "            x_2 = feats[sizes_1.item() : sizes_1.item() + sizes_2.item(), :]\n",
    "            x_pooled_1, edge_index_pooled_1, _, _, perm1, _ = self.topk_pooling(\n",
    "                x_1,\n",
    "                edge_index_1,\n",
    "            )\n",
    "            x_pooled_2, edge_index_pooled_2, _, _, perm2, _ = self.topk_pooling(\n",
    "                x_2,\n",
    "                edge_index_2,\n",
    "            )\n",
    "            self.topk_outputs.append(\n",
    "                (\n",
    "                    (x_pooled_1, edge_index_pooled_1, perm1),\n",
    "                    (x_pooled_2, edge_index_pooled_2, perm2),\n",
    "                )\n",
    "            )\n",
    "            self.layer_cross_attentions.append((cross_graph_attention, a_x, a_y))\n",
    "            self.layer_outputs.append((x_1, x_2))\n",
    "\n",
    "        feats = self.aggregator(feats, edge_index, batch)\n",
    "        return feats, edge_index, batch\n",
    "\n",
    "    def combine_pair_embedding(\n",
    "        self, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "    ):\n",
    "        feats = torch.cat([feats_1, feats_2], dim=0)\n",
    "        max_node_idx_1 = sizes_1.sum()\n",
    "        edge_index_2_offset = edge_index_2 + max_node_idx_1\n",
    "        edge_index = torch.cat([edge_index_1, edge_index_2_offset], dim=1)\n",
    "        batch = create_batch(torch.cat([sizes_1, sizes_2], dim=0))\n",
    "        feats, edge_index, batch = (\n",
    "            feats.to(self.args.device),\n",
    "            edge_index.to(self.args.device),\n",
    "            batch.to(self.args.device),\n",
    "        )\n",
    "        return feats, edge_index, batch\n",
    "\n",
    "    def forward(self, feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2):\n",
    "        self.layer_outputs = []\n",
    "        self.layer_cross_attentions = []\n",
    "        self.topk_outputs = []\n",
    "        self.mincut = []\n",
    "        feats, edge_index, batch = self.combine_pair_embedding(\n",
    "            feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "        )\n",
    "        emb, _, _ = self.compute_emb(\n",
    "            feats, edge_index, batch, sizes_1, sizes_2, edge_index_1, edge_index_2\n",
    "        )\n",
    "        emb_1 = emb[: emb.shape[0] // 2, :]\n",
    "        emb_2 = emb[emb.shape[0] // 2 :, :]\n",
    "\n",
    "        best_acc1, best_acc2 = 0.0, 0.0\n",
    "        cluster1, cluster2 = None, None\n",
    "        for i in range(len(self.topk_outputs)):\n",
    "            (\n",
    "                (x_pooled_1, edge_index_pooled_1, perm1),\n",
    "                (x_pooled_2, edge_index_pooled_2, perm2),\n",
    "            ) = self.topk_outputs[i]\n",
    "            acc1 = len(set(range(8)) & set(perm1.tolist()))\n",
    "            acc2 = len(set(range(8)) & set(perm2.tolist()))\n",
    "            if acc1 > best_acc1:\n",
    "                cluster1 = Data(x=x_pooled_1, edge_index=edge_index_pooled_1)\n",
    "                best_acc1 = acc1\n",
    "            if acc2 > best_acc2:\n",
    "                cluster2 = Data(x=x_pooled_2, edge_index=edge_index_pooled_2)\n",
    "                best_acc2 = acc2\n",
    "\n",
    "        return emb_1, emb_2, cluster1, cluster2\n",
    "\n",
    "    def compute_metrics(self, emb_1, emb_2, labels):\n",
    "        distances = torch.norm(emb_1 - emb_2, p=2, dim=1)\n",
    "        loss = F.relu(self.margin - labels * (1 - distances)).mean()\n",
    "        predicted_similar = torch.where(\n",
    "            distances < self.args.margin,\n",
    "            torch.ones_like(labels),\n",
    "            -torch.ones_like(labels),\n",
    "        )\n",
    "        acc = (predicted_similar == labels).float().mean()\n",
    "        metrics = {\"loss\": loss, \"acc\": acc}\n",
    "        return metrics\n",
    "\n",
    "    def init_metric_dict(self):\n",
    "        return {\"acc\": -1, \"f1\": -1}\n",
    "\n",
    "    def has_improved(self, m1, m2):\n",
    "        return m1[\"acc\"] < m2[\"acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = TUDataset(\n",
    "#     root=\"data\", name=\"MUTAG\", use_node_attr=True, transform=NormalizeFeatures()\n",
    "# )\n",
    "\n",
    "dataset = GraphDataset(torch.load(\"data/cycle_line_star_complete_1.pt\"))\n",
    "\n",
    "small_graphs, medium_graphs, large_graphs, classes = analyze_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Args:\n",
    "#     def __init__(self):\n",
    "#         self.dim = 16\n",
    "#         self.feat_dim = dataset.num_features\n",
    "#         self.num_layers = 7\n",
    "#         self.margin = 0.4\n",
    "#         self.lr = 0.001\n",
    "#         self.n_classes = dataset.num_classes\n",
    "#         self.batch_size = 128\n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         self.n_clusters = 8\n",
    "#         self.num_pairs = 3000\n",
    "\n",
    "\n",
    "# args = Args()\n",
    "\n",
    "# pairs, labels = create_graph_pairs(dataset, args.num_pairs)\n",
    "# # pair_dataset = [(pair, label) for pair, label in zip(pairs, labels)]\n",
    "# # train_loader = DataLoader(\n",
    "# #     pair_dataset,\n",
    "# #     batch_size=args.batch_size,\n",
    "# #     shuffle=True,\n",
    "# #     collate_fn=collate_graph_pairs,\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, pairs, labels, batch_size):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    losses = []\n",
    "    accs = []\n",
    "\n",
    "    def get_params(model):\n",
    "        return {name: param.clone() for name, param in model.named_parameters()}\n",
    "\n",
    "    initial_params = get_params(model)\n",
    "\n",
    "    for i in range(len(pairs)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        graph1, graph2 = pairs[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "        feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "        sizes_1 = torch.tensor([graph1.num_nodes])\n",
    "        sizes_2 = torch.tensor([graph2.num_nodes])\n",
    "\n",
    "        emb_1, emb_2, _, _ = model(\n",
    "            feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "        )\n",
    "\n",
    "        metrics = model.compute_metrics(emb_1, emb_2, torch.tensor([label]))\n",
    "        loss = metrics[\"loss\"]\n",
    "        acc = metrics[\"acc\"]\n",
    "\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "\n",
    "        if i % batch_size == 0 and i > 0:\n",
    "            batch_loss = torch.mean(torch.stack(losses))\n",
    "            batch_acc = torch.mean(torch.stack(accs))\n",
    "            losses = []\n",
    "            accs = []\n",
    "            train_losses.append(batch_loss.detach().numpy())\n",
    "            train_accuracies.append(batch_acc.detach().numpy())\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            # if i % 100 * batch_size == 0:\n",
    "            #     print(\n",
    "            #         f\"Epoch: {i} - Loss: {batch_loss.item():.4f}, Acc: {batch_acc:.4f}\"\n",
    "            #     )\n",
    "\n",
    "    trained_params = get_params(model)\n",
    "\n",
    "    # for name in initial_params:\n",
    "    #     initial_param = initial_params[name]\n",
    "    #     trained_param = trained_params[name]\n",
    "    #     if not torch.equal(initial_param, trained_param):\n",
    "    #         print(f\"Parameter {name} has changed.\")\n",
    "    #     else:\n",
    "    #         print(f\"Parameter {name} has NOT changed.\")\n",
    "\n",
    "    # plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.plot(train_losses, label=\"Training Loss\")\n",
    "    # plt.title(\"Loss over Epochs\")\n",
    "    # plt.xlabel(\"Epochs\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "    # plt.legend()\n",
    "\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "    # plt.title(\"Accuracy over Epochs\")\n",
    "    # plt.xlabel(\"Epochs\")\n",
    "    # plt.ylabel(\"Accuracy\")\n",
    "    # plt.legend()\n",
    "\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GraphMatchingNetwork(args).to(args.device)\n",
    "# optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)\n",
    "\n",
    "# filename = f\"checkpoints/checkpoint_{dataset.name.lower()}_{args.dim}_{args.num_layers}_{args.margin}_{args.lr}_{args.batch_size}.pth\"\n",
    "\n",
    "# if path.exists(filename):\n",
    "#     checkpoint = torch.load(filename)\n",
    "#     model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "#     optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "#     for state in optimizer.state.values():\n",
    "#         for k, v in state.items():\n",
    "#             if isinstance(v, torch.Tensor):\n",
    "#                 state[k] = v.to(args.device)\n",
    "#     model.to(args.device)\n",
    "# else:\n",
    "#     train(model, optimizer, pairs, labels, args.batch_size)\n",
    "#     # torch.save(\n",
    "#     #     {\n",
    "#     #         \"model_state_dict\": model.state_dict(),\n",
    "#     #         \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "#     #     },\n",
    "#     #     filename,\n",
    "#     # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, title=\"\", layers=3):\n",
    "    class_clusters = []\n",
    "    class_accs = []\n",
    "    for i in range(4):\n",
    "        c = f\"class_{str(i)}\"\n",
    "        idx1 = random.sample(range(len(classes[c])), 1)[0]\n",
    "        idx2 = random.sample(range(len(classes[c])), 1)[0]\n",
    "        graph1, graph2 = classes[c][idx1], classes[c][idx2]\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "        feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "        sizes_1 = torch.tensor([len(graph1.x)])\n",
    "        sizes_2 = torch.tensor([len(graph2.x)])\n",
    "        _, _, cluster1, cluster2 = model(\n",
    "            feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "        )\n",
    "\n",
    "        clusters = []\n",
    "        accs = []\n",
    "\n",
    "        for i in range(len(model.topk_outputs)):\n",
    "            (\n",
    "                (x_pooled_1, edge_index_pooled_1, perm1),\n",
    "                (x_pooled_2, edge_index_pooled_2, perm2),\n",
    "            ) = model.topk_outputs[i]\n",
    "            clusters.append(\n",
    "                (\n",
    "                    Data(x=x_pooled_1, edge_index=edge_index_pooled_1),\n",
    "                    Data(x=x_pooled_2, edge_index=edge_index_pooled_2),\n",
    "                )\n",
    "            )\n",
    "            accs.append(\n",
    "                (\n",
    "                    len(set(range(8)) & set(perm1.tolist())),\n",
    "                    len(set(range(8)) & set(perm2.tolist())),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # if c == \"class_0\":\n",
    "        #     print(is_cycle(cluster1), is_cycle(cluster2))\n",
    "        # elif c == \"class_1\":\n",
    "        #     print(is_complete(cluster1), is_complete(cluster2))\n",
    "        # elif c == \"class_2\":\n",
    "        #     print(is_line(cluster1), is_line(cluster2))\n",
    "        # elif c == \"class_3\":\n",
    "        #     print(is_star(cluster1), is_star(cluster2))\n",
    "\n",
    "        acc = list(itertools.chain.from_iterable(zip(*accs)))\n",
    "        class_accs.extend(acc)\n",
    "\n",
    "        cs = list(itertools.chain.from_iterable(zip(*clusters)))\n",
    "        class_clusters.extend(cs)\n",
    "        plot_graph_pair(cluster1, cluster2)\n",
    "\n",
    "    plot_all_classes(class_clusters, class_accs, title, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_test(model, print_results=True):\n",
    "    class_clusters = []\n",
    "    class_accs = []\n",
    "    correct_class0 = 0\n",
    "    correct_class1 = 0\n",
    "    correct_class2 = 0\n",
    "    correct_class3 = 0\n",
    "    best_class0 = 0\n",
    "    best_class1 = 0\n",
    "    best_class2 = 0\n",
    "    best_class3 = 0\n",
    "    for _ in range(500):\n",
    "        for i in range(4):\n",
    "            c = f\"class_{str(i)}\"\n",
    "            idx1 = random.sample(range(len(classes[c])), 1)[0]\n",
    "            idx2 = random.sample(range(len(classes[c])), 1)[0]\n",
    "            graph1, graph2 = classes[c][idx1], classes[c][idx2]\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            feats_1, edge_index_1 = graph1.x, graph1.edge_index\n",
    "            feats_2, edge_index_2 = graph2.x, graph2.edge_index\n",
    "            sizes_1 = torch.tensor([len(graph1.x)])\n",
    "            sizes_2 = torch.tensor([len(graph2.x)])\n",
    "            _, _, cluster1, cluster2 = model(\n",
    "                feats_1, edge_index_1, feats_2, edge_index_2, sizes_1, sizes_2\n",
    "            )\n",
    "\n",
    "            if c == \"class_0\":\n",
    "                if not is_cycle(cluster1):\n",
    "                    plot_graph(cluster1)\n",
    "                if not is_cycle(cluster2):\n",
    "                    plot_graph(cluster2)\n",
    "                correct_class0 += is_cycle(cluster1) + is_cycle(cluster2)\n",
    "                best_class0 += any([is_cycle(cluster1), is_cycle(cluster2)])\n",
    "            elif c == \"class_1\":\n",
    "                if not is_complete(cluster1):\n",
    "                    plot_graph(cluster1)\n",
    "                if not is_complete(cluster2):\n",
    "                    plot_graph(cluster2)\n",
    "                correct_class1 += is_complete(cluster1) + is_complete(cluster2)\n",
    "                best_class1 += any([is_complete(cluster1), is_complete(cluster2)])\n",
    "            elif c == \"class_2\":\n",
    "                if not is_line(cluster1):\n",
    "                    plot_graph(cluster1)\n",
    "                if not is_line(cluster2):\n",
    "                    plot_graph(cluster2)\n",
    "                correct_class2 += is_line(cluster1) + is_line(cluster2)\n",
    "                best_class2 += any([is_line(cluster1), is_line(cluster2)])\n",
    "            elif c == \"class_3\":\n",
    "                if not is_star(cluster1):\n",
    "                    plot_graph(cluster1)\n",
    "                if not is_star(cluster2):\n",
    "                    plot_graph(cluster2)\n",
    "                correct_class3 += is_star(cluster1) + is_star(cluster2)\n",
    "                best_class3 += any([is_star(cluster1), is_star(cluster2)])\n",
    "\n",
    "    class0_acc = correct_class0 / 10\n",
    "    class1_acc = correct_class1 / 10\n",
    "    class2_acc = correct_class2 / 10\n",
    "    class3_acc = correct_class3 / 10\n",
    "    overall_acc = (class0_acc + class1_acc + class2_acc + class3_acc) / 4\n",
    "    best_class0_acc = best_class0 / 5\n",
    "    best_class1_acc = best_class1 / 5\n",
    "    best_class2_acc = best_class2 / 5\n",
    "    best_class3_acc = best_class3 / 5\n",
    "    best_overall_acc = (\n",
    "        best_class0_acc + best_class1_acc + best_class2_acc + best_class3_acc\n",
    "    ) / 4\n",
    "    if print_results:\n",
    "        print(f\"Correct cycle predictions: {class0_acc:.1f}%\")\n",
    "        print(f\"Correct complete predictions: {class1_acc:.1f}%\")\n",
    "        print(f\"Correct line predictions: {class2_acc:.1f}%\")\n",
    "        print(f\"Correct star predictions: {class3_acc:.1f}%\")\n",
    "        print(f\"Overall accuracy: {overall_acc:.1f}%\")\n",
    "    return (class0_acc, class1_acc, class2_acc, class3_acc, overall_acc), (\n",
    "        best_class0_acc,\n",
    "        best_class1_acc,\n",
    "        best_class2_acc,\n",
    "        best_class3_acc,\n",
    "        best_overall_acc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewArgs:\n",
    "    def __init__(self, dim, num_layers, margin, lr, batch_size, num_pairs):\n",
    "        self.dim = dim\n",
    "        self.feat_dim = dataset.num_features\n",
    "        self.num_layers = num_layers\n",
    "        self.margin = margin\n",
    "        self.lr = lr\n",
    "        self.n_classes = dataset.num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.n_clusters = 8\n",
    "        self.num_pairs = num_pairs\n",
    "\n",
    "\n",
    "# l = grey + purple\n",
    "# results = {}\n",
    "# for hyperparams in l:\n",
    "#     newargs = NewArgs(*hyperparams)\n",
    "#     m = GraphMatchingNetwork(newargs)\n",
    "#     o = Adam(m.parameters(), lr=newargs.lr, weight_decay=1e-5)\n",
    "#     p, l = create_graph_pairs(dataset, newargs.num_pairs)\n",
    "#     train(m, o, p, l, newargs.batch_size)\n",
    "#     results[str(hyperparams)] = acc_test(m, False)\n",
    "\n",
    "# n_groups = len(results)\n",
    "# n_categories = len(results[0])\n",
    "\n",
    "# # Creating a figure and an axis object\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Creating an index for each tick position\n",
    "# index = np.arange(n_groups)\n",
    "\n",
    "# # The bar width\n",
    "# bar_width = 0.15\n",
    "\n",
    "# # The opacity for the bars\n",
    "# opacity = 0.8\n",
    "\n",
    "# # Plotting data\n",
    "# for i in range(n_categories):\n",
    "#     # Extracting the i-th percentage from each group\n",
    "#     percentages = [t[i] for t in results]\n",
    "\n",
    "#     # Plotting\n",
    "#     plt.bar(\n",
    "#         index + i * bar_width,\n",
    "#         percentages,\n",
    "#         bar_width,\n",
    "#         alpha=opacity,\n",
    "#         label=f\"Category {i+1}\",\n",
    "#     )\n",
    "\n",
    "# # Adding labels, title and axes ticks\n",
    "# plt.xlabel(\"Group\")\n",
    "# plt.ylabel(\"Percentages\")\n",
    "# plt.title(\"Percentages by Group and Category\")\n",
    "# plt.xticks(\n",
    "#     index + bar_width * (n_categories - 1) / 2,\n",
    "#     [f\"Group {i+1}\" for i in range(n_groups)],\n",
    "# )\n",
    "# plt.legend()\n",
    "\n",
    "# # Display the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best = sorted(results.items(), key=lambda item: item[1][-1], reverse=True)[:10]\n",
    "# print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = [\n",
    "    # (16, 4, 0.1, 0.001, 128, 500),\n",
    "    # (16, 6, 0.4, 0.001, 32, 500),\n",
    "    (16, 7, 0.3, 0.0001, 16, 1000),\n",
    "    # (32, 4, 0.2, 0.0005, 16, 1000),\n",
    "    (16, 8, 0.3, 0.001, 128, 500),\n",
    "    (16, 7, 0.3, 0.001, 64, 3000),\n",
    "    # (16, 6, 0.1, 0.0005, 16, 500),\n",
    "    # (16, 8, 0.3, 0.0001, 64, 500),\n",
    "    (16, 8, 0.4, 0.0001, 128, 1000),\n",
    "    # (16, 6, 0.5, 0.01, 64, 1000),\n",
    "]\n",
    "\n",
    "dims = [32]\n",
    "num_layers_s = [3, 4, 5, 6, 7, 8]\n",
    "# num_layers_s = [8]\n",
    "margins = [0.1, 0.2, 0.3, 0.4, 0.5, 1]\n",
    "lrs = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "num_pairs_s = [500, 1000, 2000, 3000]\n",
    "\n",
    "\n",
    "results = {}\n",
    "for hyperparams in itertools.product(\n",
    "    dims, num_layers_s, margins, lrs, batch_sizes, num_pairs_s\n",
    "):\n",
    "    newargs = NewArgs(*hyperparams)\n",
    "    m = GraphMatchingNetwork(newargs)\n",
    "    o = Adam(m.parameters(), lr=newargs.lr, weight_decay=1e-5)\n",
    "    p, l = create_graph_pairs(dataset, newargs.num_pairs)\n",
    "    train(m, o, p, l, newargs.batch_size)\n",
    "    acc = acc_test(m, False)\n",
    "    if acc[-1] > 85.0:\n",
    "        print(f\"{hyperparams}: {acc}\")\n",
    "    results[str(hyperparams)] = acc\n",
    "\n",
    "torch.save(results, \"results.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_32 = [\n",
    "    (32, 4, 0.4, 0.001, 64, 500),\n",
    "    (32, 5, 0.1, 0.01, 64, 3000),\n",
    "    (32, 5, 0.2, 0.01, 16, 3000),\n",
    "    (32, 5, 0.2, 0.01, 32, 3000),\n",
    "    (32, 5, 0.4, 0.01, 32, 1000),\n",
    "    (32, 6, 0.1, 0.0001, 32, 2000),\n",
    "    (32, 6, 0.1, 0.0001, 64, 3000),\n",
    "    (32, 6, 0.1, 0.0001, 128, 500),\n",
    "    (32, 6, 0.2, 0.01, 16, 3000),\n",
    "    (32, 6, 0.2, 0.001, 64, 500),\n",
    "    (32, 6, 0.2, 0.0001, 32, 2000),\n",
    "    (32, 6, 0.4, 0.001, 128, 2000),\n",
    "    (32, 6, 0.5, 0.001, 16, 3000),\n",
    "    (32, 6, 0.5, 0.0001, 128, 3000),\n",
    "    (32, 7, 0.2, 0.01, 16, 3000),\n",
    "    (32, 7, 0.2, 0.01, 64, 500),\n",
    "    (32, 7, 0.2, 0.01, 128, 3000),\n",
    "    (32, 7, 0.2, 0.001, 128, 500),\n",
    "    (32, 7, 0.2, 0.0001, 64, 3000),\n",
    "    (32, 7, 0.3, 0.01, 64, 1000),\n",
    "    (32, 7, 0.3, 0.0001, 32, 1000),\n",
    "    (32, 7, 0.4, 0.01, 16, 2000),\n",
    "    (32, 7, 0.4, 0.0001, 32, 500),\n",
    "    (32, 7, 0.4, 0.0001, 64, 2000),\n",
    "    (32, 7, 0.4, 0.0001, 128, 3000),\n",
    "    (32, 7, 0.5, 0.01, 32, 500),\n",
    "    (32, 7, 0.5, 0.01, 32, 3000),\n",
    "    (32, 7, 0.5, 0.01, 128, 1000),\n",
    "    (32, 7, 0.5, 0.0001, 32, 500),\n",
    "    (32, 7, 0.5, 0.0001, 128, 3000),\n",
    "    (32, 7, 1, 0.01, 64, 1000),\n",
    "    (32, 7, 1, 0.001, 128, 1000),\n",
    "    (32, 7, 1, 0.001, 128, 2000),\n",
    "    (32, 8, 0.2, 0.01, 128, 1000),\n",
    "    (32, 8, 0.2, 0.0001, 32, 2000),\n",
    "    (32, 8, 0.3, 0.01, 64, 2000),\n",
    "    (32, 8, 0.3, 0.0001, 64, 500),\n",
    "    (32, 8, 0.3, 0.0001, 64, 1000),\n",
    "    (32, 8, 0.4, 0.01, 16, 3000),\n",
    "    (32, 8, 0.4, 0.0001, 128, 2000),\n",
    "    (32, 8, 0.5, 0.01, 32, 3000),\n",
    "    (32, 8, 0.5, 0.01, 64, 3000),\n",
    "    (32, 8, 0.5, 0.001, 32, 2000),\n",
    "    (32, 8, 0.5, 0.001, 64, 3000),\n",
    "    (32, 8, 0.5, 0.001, 128, 500),\n",
    "    (32, 8, 0.5, 0.0001, 32, 2000),\n",
    "    (32, 8, 1, 0.001, 32, 500),\n",
    "    (32, 8, 1, 0.0001, 64, 2000),\n",
    "    (32, 8, 1, 0.0001, 64, 3000),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = {}\n",
    "for hyperparams in hp_32:\n",
    "    newargs = NewArgs(*hyperparams)\n",
    "    accs = []\n",
    "    for _ in range(10):\n",
    "        m = GraphMatchingNetwork(newargs)\n",
    "        o = Adam(m.parameters(), lr=newargs.lr, weight_decay=1e-5)\n",
    "        p, l = create_graph_pairs(dataset, newargs.num_pairs)\n",
    "        train(m, o, p, l, newargs.batch_size)\n",
    "        acc, best_accs = acc_test(m, False)\n",
    "        accs.append(acc)\n",
    "    mean_acc = tuple(sum(t) / len(accs) for t in zip(*accs))\n",
    "    new_results[str(hyperparams)] = mean_acc\n",
    "\n",
    "torch.save(new_results, \"new_results.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_best = sorted(new_results.items(), key=lambda item: item[1][-1], reverse=True)[:10]\n",
    "# print(new_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = {}\n",
    "for hyperparams in hp_32:\n",
    "    newargs = NewArgs(*hyperparams)\n",
    "    m = GraphMatchingNetwork(newargs)\n",
    "    o = Adam(m.parameters(), lr=newargs.lr, weight_decay=1e-5)\n",
    "    p, l = create_graph_pairs(dataset, newargs.num_pairs)\n",
    "    train(m, o, p, l, newargs.batch_size)\n",
    "    acc, best_acc = acc_test(m, False)\n",
    "    best_results[str(hyperparams)] = (acc, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in best_results.items():\n",
    "    value = list(value)\n",
    "    inner_tuple = list(value[-1])\n",
    "    inner_tuple[-1] = inner_tuple[-1] / 5\n",
    "    value[-1] = tuple(inner_tuple)\n",
    "    value = tuple(value)\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = sorted(\n",
    "    best_results.items(), key=lambda item: item[1][-1][-1], reverse=True\n",
    ")\n",
    "\n",
    "top_10_results = sorted_results[:10]\n",
    "\n",
    "for key, value in top_10_results:\n",
    "    value = list(value)\n",
    "    inner_tuple = list(value[-1])\n",
    "    inner_tuple[-1] = inner_tuple[-1] / 5\n",
    "    value[-1] = tuple(inner_tuple)\n",
    "    value = tuple(value)\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = [\n",
    "    (32, 7, 0.5, 0.0001, 128, 3000),\n",
    "    (32, 7, 0.2, 0.01, 64, 500),\n",
    "    (32, 5, 0.2, 0.01, 32, 3000),\n",
    "    (32, 8, 0.3, 0.0001, 64, 1000),\n",
    "    (32, 7, 0.5, 0.01, 32, 3000),\n",
    "    (32, 7, 0.4, 0.0001, 32, 500),\n",
    "    (32, 5, 0.1, 0.01, 64, 3000),\n",
    "    (32, 7, 0.4, 0.0001, 128, 3000),\n",
    "    (32, 7, 0.5, 0.01, 128, 1000),\n",
    "    (32, 8, 0.3, 0.0001, 64, 500),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyperparams in [(32, 7, 0.5, 0.0001, 128, 3000)]:\n",
    "    newargs = NewArgs(*hyperparams)\n",
    "    m = GraphMatchingNetwork(newargs)\n",
    "    o = Adam(m.parameters(), lr=newargs.lr, weight_decay=1e-5)\n",
    "    p, l = create_graph_pairs(dataset, newargs.num_pairs)\n",
    "    train(m, o, p, l, newargs.batch_size)\n",
    "    acc, best_acc = acc_test(m, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Args:\n",
    "#     def __init__(self, dim, num_layers, margin, lr, batch_size, num_pairs):\n",
    "#         self.dim = dim\n",
    "#         self.feat_dim = dataset.num_features\n",
    "#         self.num_layers = num_layers\n",
    "#         self.margin = margin\n",
    "#         self.lr = lr\n",
    "#         self.n_classes = dataset.num_classes\n",
    "#         self.batch_size = batch_size\n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         self.n_clusters = 8\n",
    "#         self.num_pairs = num_pairs\n",
    "\n",
    "\n",
    "# dims = [16]\n",
    "# # num_layers_s = [4, 5, 6, 7, 8]\n",
    "# num_layers_s = [8]\n",
    "# margins = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# lrs = [0.01, 0.001, 0.0001]\n",
    "# batch_sizes = [16, 32, 64, 128]\n",
    "# num_pairs_s = [500, 1000, 2000, 3000]\n",
    "\n",
    "\n",
    "# for hyperparams in itertools.product(\n",
    "#     dims, num_layers_s, margins, lrs, batch_sizes, num_pairs_s\n",
    "# ):\n",
    "#     args = Args(*hyperparams)\n",
    "#     m = GraphMatchingNetwork(args)\n",
    "#     o = Adam(m.parameters(), lr=args.lr, weight_decay=1e-5)\n",
    "#     p, l = create_graph_pairs(dataset, args.num_pairs)\n",
    "#     train(m, o, p, l, args.batch_size)\n",
    "#     title = f\"Dim:{args.dim},NumLayers:{args.num_layers},Margin:{args.margin},LearningRate:{args.lr},BatchSize:{args.batch_size},NumPairs:{args.num_pairs}\"\n",
    "#     test(m, title, args.num_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
